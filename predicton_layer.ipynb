{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THE PREDICTION LAYER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we get to the final prediction of the probability that the element will appear in the next set, according to the previous user's states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y}_i = sigmoid(E_i^{update} \\cdot w_0 + b_0)$, where $w_0 \\in \\mathbb{R}^F$ and $b_0 \\in \\mathbb{R}$ are the trainable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now put all the things we defined before together and construct the final class `temporal_set_prediction`, which just calls all the classes we have defined in that order. <br>\n",
    "Let's remember, first we construct the embeddings with `weighted_GCN` and construct the blocks with `stacked_weighted_GCN_blocks`. Next we get the attention with `masked_self_attention` and aggregate them using `aggregate_nodes_temporal_feature`. Lastly we nees the matrix $E$ using `global_gated_update` and perform a final linear transformation to get the predictions $\\hat{y}_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dgl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6d506d6bae42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdgl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweighted_graph_conv\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstacked_weighted_GCN_blocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweighted_GCN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked_self_attention\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmasked_self_attention\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dgl'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import dgl\n",
    "from model.weighted_graph_conv import stacked_weighted_GCN_blocks, weighted_GCN\n",
    "from model.masked_self_attention import masked_self_attention\n",
    "from model.global_gated_update import global_gated_update\n",
    "from model.aggregate_nodes_temporal_feature import aggregate_nodes_temporal_feature\n",
    "\n",
    "\n",
    "class temporal_set_prediction(nn.Module):\n",
    "\n",
    "    def __init__(self, items_total, item_embedding_dim):\n",
    "        \"\"\"\n",
    "        :param items_total: int\n",
    "        :param item_embedding_dim: int\n",
    "        :param n_heads: int\n",
    "        :param attention_aggregate: sre\n",
    "        \"\"\"\n",
    "        super(temporal_set_prediction, self).__init__()\n",
    "\n",
    "        self.item_embedding = nn.Embedding(items_total, item_embedding_dim)\n",
    "\n",
    "        self.item_embedding_dim = item_embedding_dim\n",
    "        self.items_total = items_total\n",
    "        self.stacked_gcn = stacked_weighted_GCN_blocks([weighted_GCN(item_embedding_dim,\n",
    "                                                                     [item_embedding_dim],\n",
    "                                                                     item_embedding_dim)])\n",
    "\n",
    "        self.masked_self_attention = masked_self_attention(input_dim=item_embedding_dim,\n",
    "                                                           output_dim=item_embedding_dim)\n",
    "\n",
    "        self.aggregate_nodes_temporal_feature = aggregate_nodes_temporal_feature(item_embed_dim=item_embedding_dim)\n",
    "\n",
    "        self.global_gated_update = global_gated_update(items_total=items_total,\n",
    "                                                       item_embedding=self.item_embedding)\n",
    "\n",
    "        self.fc_output = nn.Linear(item_embedding_dim, 1)\n",
    "\n",
    "    def forward(self, graph: dgl.DGLGraph, nodes_feature: torch.Tensor, edges_weight: torch.Tensor,\n",
    "                lengths: torch.Tensor, nodes: torch.Tensor, users_frequency: torch.Tensor):\n",
    "        \"\"\"\n",
    "        :param graph: batched graphs, with the total number of nodes is `node_num`,\n",
    "                        including `batch_size` disconnected subgraphs\n",
    "        :param nodes_feature:  [n_1+n_2+..., F]\n",
    "        :param edges_weight: [T_max, n_1^2+n_2^2+...]\n",
    "        :param lengths: [batch_size, ]\n",
    "        :param nodes: [n_1+n_2+..., ]\n",
    "        :param users_frequency: (batch, items_total), for frequency calculation\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # perform weighted gcn on dynamic graphs (n_1+n_2+..., T_max, item_embed_dim)\n",
    "        nodes_output = self.stacked_gcn(graph, nodes_feature, edges_weight)\n",
    "\n",
    "        # self-attention in time dimension, (n_1+n_2+..., T_max,  item_embed_dim)\n",
    "        nodes_output = self.masked_self_attention(nodes_output)\n",
    "        # aggregate node features in temporal dimension, (n_1+n_2+..., item_embed_dim)\n",
    "        nodes_output = self.aggregate_nodes_temporal_feature(graph, lengths, nodes_output)\n",
    "\n",
    "        # (batch_size, items_total, item_embed_dim)\n",
    "        nodes_output = self.global_gated_update(graph, nodes, nodes_output)\n",
    "\n",
    "        # (batch_size, items_total)\n",
    "        output = self.fc_output(nodes_output).squeeze(dim=-1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

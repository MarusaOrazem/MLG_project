{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather information fusing components consist of two parts.\n",
    "First is the element representation matrix $\\textbf{E}$, which could be seen as a static representation of all the element and it is shared with all users. This is an advantage, as we could use it also for sparse data and we do not need any additional attributes for the users.\n",
    "Second is the compact representation of element w.r.t $u_i$ we constructed in the previous step, $\\mathbb{Z}_i = \\{ z_{i,1},...,z_{i,|\\mathcal{V}_i|} \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use $E_i$ to denote the hidden state of user $i$, which is initializes as $E$. The most recent state $E^{update}_{i, I(j)}$ is achieved by updating the user state $E_i$ iteratively as follows:,\n",
    "$E^{update}_{i, I(j)} = (1- \\beta_{i,I(j)} \\cdot \\gamma_{I(j)}) \\cdot E_{i,I(j)} + (\\beta_{i,I(j)} \\cdot \\gamma_{I(j)}) \\cdot z_{i,j}$, where $I(\\cdot)$ is a function that maps element $v_{i,j}$ to its corresponding index in $E_i, \\beta_{i,j}$ and $\\gamma_j$ are the j-th dimention of $\\beta_i$ and $\\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class global_gated_update(nn.Module):\n",
    "\n",
    "    def __init__(self, items_total, item_embedding):\n",
    "        super(global_gated_update, self).__init__()\n",
    "        \n",
    "        # TODO: ali items_total predstavlja vse možne izdelke (za vse houdeholde, kot je v E), ali samo za trenutni houdehold\n",
    "        # Mislim, da so to VSI IZDELKI!!\n",
    "        # items_total: vsi možni izdelki (za vse householde)\n",
    "        self.items_total = items_total\n",
    "        \n",
    "        # item_embedding: matrika E (statična matrika), v kateri so neke reprezentacije za vse možne izdelke (na začetku inicializirana)\n",
    "        self.item_embedding = item_embedding\n",
    "\n",
    "        # Uteži za updejtanje\n",
    "        self.gamma = nn.Parameter(torch.rand(items_total, 1), requires_grad=True)\n",
    "\n",
    "    def forward(self, graph, nodes, nodes_output):\n",
    "        \"\"\"\n",
    "        :param graph: batched graphs, with the total number of nodes is `node_num`,\n",
    "                        including `batch_size` disconnected subgraphs\n",
    "        :param nodes: tensor (n_1+n_2+..., )\n",
    "        :param nodes_output: the output of self-attention model in time dimension, (n_1+n_2+..., F)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \n",
    "        nums_nodes, id = graph.batch_num_nodes(), 0\n",
    "        # .batch_num_nodes(): vrne število nodov za vsak graf v batchu\n",
    "        # Npr seznam [število nodov v prvem grafu, število nodov v drugem grafu, ..., vseh grafov je toliko kot je košaric za]\n",
    "        # A ni število nodov pri vseh grafih za isti household enako? A je tuki šel s to for zanko samo zaradi tega, ker je funkcija .batch_num_nodes() komot?\n",
    "        \n",
    "        # E_iI(j) = Iz matrike E vzame samo tiste node oz izdelke, ki se pojavijo pri pozameznem householdu\n",
    "        # To je ista matrika kot E na začetku -- zakaj potem spreminjamo in ne vzamemo kar item_embedding? \n",
    "        items_embedding = self.item_embedding(torch.tensor([i for i in range(self.items_total)]) #.to(nodes.device))\n",
    "        batch_embedding = []\n",
    "                                              \n",
    "        # Sprehodimo se čez vsak graf za posamezen houdehold \n",
    "        # Z eno for zanko se sprehodimo čez en graf (j od 1 do |vi|, okno velikosti |vi|)     -- (d:id + num_nodes): na tak način gremo čez vse izdelke v grafu\n",
    "        # Se ubistvu sprehajamo po grafih                                     \n",
    "        for num_nodes in nums_nodes:\n",
    "                                              \n",
    "            # TUKI NOTR JE VSE NA NIVOJU GRAFOV... extractamo izdelke iz trenutnega grafa v for zanki!                                  \n",
    "                                              \n",
    "            # tensor, shape, (user_nodes, item_embed_dim)                                      \n",
    "            # output_node_features: z_ij (vzamemo vse take)                                  \n",
    "            output_node_features = nodes_output[id:id + num_nodes, :]\n",
    "            \n",
    "            # Izdelki za trenuten graf v for zanki za trenuten householda                                 \n",
    "            output_nodes = nodes[id: id + num_nodes]\n",
    "                                              \n",
    "            # beta, tensor, (items_total, 1), indicator vector, appear item -> 1, not appear -> 0\n",
    "            # Najprej nastavimo same 0                                  \n",
    "            beta = torch.zeros(self.items_total, 1).to(nodes.device)\n",
    "            # Potem pa za tiste izdelki, ki se pojavijo, beto nastavimo na 1                                  \n",
    "            beta[output_nodes] = 1\n",
    "                                              \n",
    "            # update global embedding by gated mechanism\n",
    "            # broadcast (items_total, 1) * (items_total, item_embed_dim) -> (items_total, item_embed_dim)\n",
    "            \n",
    "            ### ----------------- UPDATE ENAČBA ----------------- ###                                  \n",
    "            # To je prvi člen enačbe (5) za update     \n",
    "            \n",
    "            # Tukaj vzamemo matriko E za vse člene, zato da embed nastavimo na enake dimenzije (da jo potem res updejtamo)                                  \n",
    "            embed = (1 - beta * self.gamma) * items_embedding.clone() \n",
    "            # appear items: (1 - self.gamma) * origin + self.gamma * update, not appear items: origin\n",
    "            # beta * self.gamma : to je 0 za izdelke, ki se ne pojavijo, in utež za izdelke, ki se pojavijo\n",
    "            # (1 - beta * self.gamma) : to je 1 (1 - 0) za izdelke, ki se ne pojavijo, in (1. utež) za izdelke, ki se pojavijo \n",
    "            # Če (1 - beta * self.gamma) množimo z matriko embeddingov za VSE izdelke, se potem tisti, ki so množeni z eni (tisti, ki se ne pojavijo) itak ne spremenijo...\n",
    "                                              \n",
    "                                              \n",
    "            # Stanje spreminjamo samo pri izdelkih, ki se pojavijo (output_nodes), ostalega ne spreminjamo  \n",
    "            # Zakaj v drugem členu tukaj nimamo bete? A zato, ker je množimo z vektorjem s samimi 1?  Moglo bi bit tkole, da bi blo prou po formuli\n",
    "                    # (beta[output_nodes] + self.gamma[output_nodes]) * output_node_features                               \n",
    "            embed[output_nodes, :] = embed[output_nodes, :] + self.gamma[output_nodes] * output_node_features\n",
    "                                              \n",
    "            batch_embedding.append(embed)\n",
    "            \n",
    "            # Se premaknemo na naslednji graf                                  \n",
    "            id += num_nodes\n",
    "                                              \n",
    "        # (B, items_total, item_embed_dim)\n",
    "        batch_embedding = torch.stack(batch_embedding)\n",
    "                                              \n",
    "        return batch_embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

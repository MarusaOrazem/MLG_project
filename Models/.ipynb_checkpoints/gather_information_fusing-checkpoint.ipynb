{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "**GATED INFORMATION FUSING**"
=======
    "**GATHER INFORMATION FUSING**"
>>>>>>> aa2a96d129e08f8cc16b2cbe93968209b9a65a3d
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Gated information fusing components consist of two parts. <br>\n",
=======
    "Gather information fusing components consist of two parts. <br>\n",
>>>>>>> aa2a96d129e08f8cc16b2cbe93968209b9a65a3d
    "First is the element representation matrix $\\textbf{E}$, which could be seen as a static representation of all the element and it is shared with all users. This is an advantage, as we could use it also for sparse data and we do not need any additional attributes for the users.<br>\n",
    "Second is the compact representation of element w.r.t $u_i$ we constructed in the previous step, $\\mathbb{Z}_i = \\{ z_{i,1},...,z_{i,|\\mathcal{V}_i|} \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use $E_i$ to denote the hidden state of user $i$, which is initializes as $E$. The most recent state $E^{update}_{i, I(j)}$ is achieved by updating the user state $E_i$ iteratively as follows:\n",
    "$E^{update}_{i, I(j)} = (1- \\beta_{i,I(j)} \\cdot \\gamma_{I(j)}) \\cdot E_{i,I(j)} + (\\beta_{i,I(j)} \\cdot \\gamma_{I(j)}) \\cdot z_{i,j}$, where $I(\\cdot)$ is a function that maps element $v_{i,j}$ to its corresponding index in $E_i, \\beta_{i,j}$ and $\\gamma_j$ are the j-th dimention of $\\beta_i$ and $\\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class global_gated_update(nn.Module):\n",
    "\n",
    "    def __init__(self, items_total, item_embedding):\n",
    "        super(global_gated_update, self).__init__()\n",
    "        self.items_total = items_total\n",
    "        self.item_embedding = item_embedding\n",
    "\n",
<<<<<<< HEAD
    "        self.gamma = nn.Parameter(torch.rand(items_total, 1), requires_grad=True)\n",
=======
    "        # alpha -> the weight for updating\n",
    "        self.alpha = nn.Parameter(torch.rand(items_total, 1), requires_grad=True)\n",
>>>>>>> aa2a96d129e08f8cc16b2cbe93968209b9a65a3d
    "\n",
    "    def forward(self, graph, nodes, nodes_output):\n",
    "        \"\"\"\n",
    "        :param graph: batched graphs, with the total number of nodes is `node_num`,\n",
    "                        including `batch_size` disconnected subgraphs\n",
    "        :param nodes: tensor (n_1+n_2+..., )\n",
    "        :param nodes_output: the output of self-attention model in time dimension, (n_1+n_2+..., F)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        nums_nodes, id = graph.batch_num_nodes(), 0\n",
<<<<<<< HEAD
    "        # dobit mormo seznam števila nodov za vse grafe za trenutni household\n",
    "        # .batch_num_nodes() : Vrne število nodov za vsak graf v batchu\n",
    "        \n",
    "        ### E_i, I(j), vzamemo samo izdelke, ki se pojavijo pri trenutnem householdu (Vi v članku = set of appearing elements in SSi)\n",
    "        items_embedding = self.item_embedding(torch.tensor([i for i in range(self.items_total)])) #.to(nodes.device))\n",
    "        \n",
    "        batch_embedding = []\n",
    "        \n",
    "        # Gremo čez \"število nodov\" za vsak graf\n",
    "        \n",
    "        # ena for zanka zajema j = 1, ... |vi| (id:id + num_nodes)\n",
    "        for num_nodes in nums_nodes:\n",
    "            \n",
    "            # tensor, shape, (user_nodes, item_embed_dim)\n",
    "            \n",
    "            # output_node_feature = z_ij (aggregation iz formule (4) v paperju)\n",
    "            # nodes_output = ZZi = {z_i1, z_i2, ..., z_i|vi|} (seznam agregacij iz formule (4))\n",
    "            output_node_features = nodes_output[id:id + num_nodes, :]\n",
    "            \n",
    "            ### Vzamemo samo tiste node, ki se pojavijo v Vi (vsi izdelki iz vseh košaric za en household)\n",
    "            output_nodes = nodes[id: id + num_nodes]\n",
    "            \n",
    "            # beta, tensor, (items_total, 1), indicator vector, appear item -> 1, not appear -> 0\n",
    "            # najprej nardimo same 0\n",
    "            beta = torch.zeros(self.items_total, 1).to(nodes.device)          \n",
    "            # Za node, ki se pojavijo v Vi nastavimo na 1 (appear item)\n",
    "            beta[output_nodes] = 1\n",
    "            \n",
    "            # update global embedding by gated mechanism\n",
    "            # broadcast (items_total, 1) * (items_total, item_embed_dim) -> (items_total, item_embed_dim)\n",
    "            \n",
    "            ### embed je prvi del update enačbe ###\n",
    "            # beta * self.gamma bo 0, kjer ni izdelkov, in utež, kjer so izdelki\n",
    "            # (1 - beta * self.gamma) bo 1 (1 - 0), kje ni izdelkov, in (1 - utež), kjer so izdelki\n",
    "            embed = (1 - beta * self.gamma) * items_embedding.clone()\n",
    "            \n",
    "            # appear items: (1 - self.gamma) * origin + self.gamma * update, not appear items: origin\n",
    "            # TODO: zakaj je v (+ self.gamma[output_nodes] * output_node_features) brez bete? V formuli je...\n",
    "            embed[output_nodes, :] = embed[output_nodes, :] + self.gamma[output_nodes] * output_node_features\n",
    "            batch_embedding.append(embed)\n",
    "            id += num_nodes\n",
    "            \n",
=======
    "        items_embedding = self.item_embedding(torch.tensor([i for i in range(self.items_total)]).to(nodes.device))\n",
    "        batch_embedding = []\n",
    "        for num_nodes in nums_nodes:\n",
    "            # tensor, shape, (user_nodes, item_embed_dim)\n",
    "            output_node_features = nodes_output[id:id + num_nodes, :]\n",
    "            # get each user's nodes\n",
    "            output_nodes = nodes[id: id + num_nodes]\n",
    "            # beta, tensor, (items_total, 1), indicator vector, appear item -> 1, not appear -> 0\n",
    "            beta = torch.zeros(self.items_total, 1).to(nodes.device)\n",
    "            beta[output_nodes] = 1\n",
    "            # update global embedding by gated mechanism\n",
    "            # broadcast (items_total, 1) * (items_total, item_embed_dim) -> (items_total, item_embed_dim)\n",
    "            embed = (1 - beta * self.alpha) * items_embedding.clone()\n",
    "            # appear items: (1 - self.alpha) * origin + self.alpha * update, not appear items: origin\n",
    "            embed[output_nodes, :] = embed[output_nodes, :] + self.alpha[output_nodes] * output_node_features\n",
    "            batch_embedding.append(embed)\n",
    "            id += num_nodes\n",
>>>>>>> aa2a96d129e08f8cc16b2cbe93968209b9a65a3d
    "        # (B, items_total, item_embed_dim)\n",
    "        batch_embedding = torch.stack(batch_embedding)\n",
    "        return batch_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.7.1"
=======
   "version": "3.8.5"
>>>>>>> aa2a96d129e08f8cc16b2cbe93968209b9a65a3d
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

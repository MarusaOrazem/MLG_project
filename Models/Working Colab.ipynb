{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TEMPORAL SET PREDICTION USING GRAPH BASED APPROACHES.\n",
    "This is the accompanying notebook to the Medium Blog entitled [\"'Don't forget the milk again!': Predicting temporal shopping sets using Graph Neural Networks\"](https://medium.com/@wwwidonja/dont-forget-the-milk-again-adc8924fdbe1). It was prepared as part of the Stanford CS224W course project @UL FRI; 2021/22 by Sara Bizjak, Maruša Oražem and Vid Stropnik. While this notebook is meant to be self-sufficient, it will be best experienced by concurrently reading the accompanying blog, linked above. Here, a more robust and theorethical overview of the model and problem at hand will be given, while the blog contextualizes the intent of the model more thoroughly and also gives a good introduction into the theory used here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Introduction\n",
    "In this notebook, we'll be implementing a model for temporal set prediction. Namely, we'll be working on the example of predicting future shopping carts by observing the purchasing habit of a given household in the past.\n",
    "\n",
    "We'll be using a model proposed by [Yu et. al. in their 2020 paper entitled \"Predicting Temporal Sets with Deep Neural Networks\"](https://dl.acm.org/doi/10.1145/3394486.3403152).\n",
    "\n",
    "As for the data, we will be using the first couple of months from the [Dunhumby's \"The Complete Journey\" dataset](https://medium.com/r?url=https%3A%2F%2Fwww.dunnhumby.com%2Fsource-files%2F). It contains transaction level data for over 2500 households in a general goods store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Problem formalization\n",
    "We are given a sequence of sets *{$s^{1}_{i}$, $s^{2}_{i}$, ... $s^{T_{max}}_{i}$}.* Each set in this sequence describes the items that were purchased by household *i* on their *t-th* visit to the general goods store, where $t \\in \\{ 1 \\dots T_{max} \\}$-th . For example, if $s^{1}_{42} = \\{\\text{item}_{26}, \\text{item}_{07}, \\text{item}_{33}\\},$ this means that the household with the identifier *42* bought three items on their first visit to this supermarket. Namely, they bought the items with identifiers 26, 07 and 33.\n",
    "\n",
    "We want to train a deep neural network that will be able to take into account both the similarities between individual items, as well as the temporal purchasing habits of individual households. With such a model, we want to predict the probable contents of a subsequent shopping basket by household *i*. More formally, we want to train the function *f* in:\n",
    "$$ s^{T_{max}+1} = f\\big{(} s^{1}_{i}, s^{2}_{i}, \\dots s^{T_{max}}_{i} | \\Theta_{f}  \\big{)}, $$ where $\\Theta_{f}$ is the set of all trainable parameters of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preparation & packages\n",
    "First, we'll import some packages that will help us get the job done.\n",
    "The main bulk of our code will be written in Pytorch (torch) and Pytorch Geometric (torch_geometric). As we'll soon see, we'll construct the representations of our sets using networks: a data structure composed of vertices and edges. To parse them efficiently, we will use the well established network library, [networkx](https://networkx.org/). We'll also extract some data from the [originalsource transaction csv file]() <font color=\"red\"> <---link needed here </font>. We'll use [pandas](https://pandas.pydata.org/) only to process that.\n",
    "\n",
    "Finally, we use some standard or utility libraries like [os](https://docs.python.org/3/library/os.html), [pickle](https://wiki.python.org/moin/UsingPickle) and [tqdm](https://github.com/tqdm/tqdm) for more efficient printouts, temporary file processing and system querying. These won't be crucial for the understanding of the text below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import path\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here, we prepare some static variables that we're going to neeed later. From the source dataset, we get a list of all unique items codes. We will also save their total number under `num_all_unique_items`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import path\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we prepare some static variables that we're going to neeed later. From the source dataset, we get a list of all unique items codes. We will also save their total number under `num_all_unique_items`.import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 24375 unique items in our dataset\n"
     ]
    }
   ],
   "source": [
    "all_unique_items = [i for i in list(pd.read_csv(os.path.join('..\\\\data\\\\', 'transaction_data_smaller.csv')).PRODUCT_ID.unique())]\n",
    "num_all_unique_items = len(all_unique_items)\n",
    "reverse_uid = {str(item_code) : idx for idx, item_code in enumerate(all_unique_items)}\n",
    "print(f'There are a total of {num_all_unique_items} unique items in our dataset')\n",
    "\n",
    "\n",
    "## we initialize these dimensions which will be explained later here, just so we can efficiently pickle our temporary files. These will be explained later.\n",
    "f0 = f1 = f2 = 32\n",
    "hidden_dims = [32, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Part 1: Constructing weighted graphs.\n",
    "As seen in the accompanying blog post, our method proposes a weighted graph convolutional network on dynamic graphs, which represent individual shopping carts. Very briefly, we need to first construct the sets from the transaction level source `.csv` file. Then, we extract the (normalized) frequency of co-occuring item pairs. Finally, we transform each shopping set to a weighted graph, where items that co-occur in it are connected with edges, weighted by the mentioned co-occurrence frequency. To each such graph, we also add self-loops of singletons that don't appear in this particular shopping cart, but are purchased by the household in some other shopping cart $s_{i}^{t}$.\n",
    "\n",
    "The first step of our approach is thus to construct these graphs. As this is quite computationally expensive, we offload the creation of these graphs into a different colab, which [you can access by clicking here](https://colab.research.google.com/drive/1vyahNq6Jo60Bg4tcMv0mfqs9Lwz5uXpC?usp=sharing).\n",
    "**In this colab, we'll work with the outputs from that colab. You can access our outputs [here]() <font color=\"red\"><--- Link needed here. </font>. Just put unzip them and put them in the root directory folder 'Graphs' to continue working.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the following cell, we construct the dictionary `shopping_per_hh`. It's keys denote the codes of all housholds in the `Graphs` folder, while it's values will be lists of `torch.data.Data` objects, representing shopping baskets from the household's discrete trips to the general goods store. Hence, at `shopping_per_hh[\"1000\"][4]`, we will find the `torch.data.Data` graph, describing household's `hid=\"1000\"` fourth tip to the store and the items they purchased.\n",
    "\n",
    "Into each `torch.data.Data` object, we will read the weighted graphs, constructed by our external colab and convert them to pytorch's way of distinguishing representing data. We'll add some aditional info to each graph - these will come handy later:\n",
    "* into `data.x`, we will save a tensor of size $(N, 1)$, where *N* is the number of nodes appearing in this graph. This tensor will denote node features, needed for our eventual GCN model to run. Because we want to treat all nodes equally, we give them equivalent features (ones).\n",
    "* into `data.id`, we will save a 'translator', which maps from the node's identifier in the graph ($i \\in \\[0, N-1\\]$) to the the identifier of the item it represents in Dunhumby's original dataset.\n",
    "* into `data.y`, we will save a sparse tensor of size $(\\hat{N}, 1)$, where $\\hat{N}$ is the number of all items, appearing across all households in the dataset. The tensor will be a multi-hot encoding and will hold ones in the indices, corresponding to the indices of the items contained in this graph's shopping basket in our static matrix representations of all items $E$. (to learn more about this, reference the Medium blog). We construct these tensors using the `reverse_uid` dictionary, that we initialized before. This dictionary will ( upon the initialization of `E` later ) serve as a reference map between the row indices of $E$ and the dataset item codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graphs from files\n",
      "torch.data.Data lists have been created for 1626 households\n"
     ]
    }
   ],
   "source": [
    "shopping_per_hh = {}\n",
    "\n",
    "#This is just a test -- we're only constructing the graphs for houshold id 22 as a proof of concept!\n",
    "print('Creating graphs from files')\n",
    "if not path.exists(path.join(\"..\\\\data\\\\pickles\", f\"shopping_per_hh_F1_{f1}_hid_{hidden_dims}.pkl.gz\")):\n",
    "    for filename in tqdm(os.listdir(\"../data/Test-Graphs/content/Graphs/\")):\n",
    "        splits = filename.split('_')\n",
    "        hh_id = splits[0]\n",
    "        if hh_id not in shopping_per_hh: shopping_per_hh[hh_id] = []\n",
    "\n",
    "\n",
    "        ## we construct a NX graph and cast it to pytorch.data.Data\n",
    "        G = nx.Graph(nx.read_pajek(os.path.join(\"../data/Test-Graphs/content/Graphs/\",filename)))\n",
    "        data = from_networkx(G)\n",
    "        \n",
    "        # We get a list of all items actually contained in the basket. These are items that are isolates in the NetworkX graph.        \n",
    "        articles_in_basket = [i for i in list(G.nodes()) if i not in list(nx.isolates(G))]\n",
    "        \n",
    "        \n",
    "        #we construct the custom data features we discussed above...\n",
    "        gt = torch.zeros(num_all_unique_items)\n",
    "        indices_in_E = [reverse_uid[i] for i in articles_in_basket]\n",
    "        gt[indices_in_E] = 1\n",
    "        gt = gt.to_sparse()\n",
    "        x = torch.ones(G.number_of_nodes(), 1)\n",
    "        \n",
    "        #..., add them to the graph representation and save the graph in the dictionary.\n",
    "        data.x = x\n",
    "        data.id = {i:code for (i, code) in zip ([i for i in range(G.number_of_nodes())], list(G.nodes()))}\n",
    "        data.y = gt\n",
    "        shopping_per_hh[hh_id].append(data)\n",
    "    \n",
    "    with open(path.join(\"..\\\\data\\\\pickles\", f\"shopping_per_hh_F1_{f1}_hid_{hidden_dims}.pkl.gz\"), \"wb\") as f:\n",
    "        pickle.dump(shopping_per_hh, f)\n",
    "\n",
    "else:\n",
    "    with open(path.join(\"..\\\\data\\\\pickles\", f\"shopping_per_hh_F1_{f1}_hid_{hidden_dims}.pkl.gz\"), \"rb\") as f:\n",
    "        shopping_per_hh = pickle.load(f)\n",
    "        \n",
    "print(f'torch.data.Data lists have been created for {len(list(shopping_per_hh.keys()))} households')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Part 2: Convolutions on weighted graphs\n",
    "Now that we have our weighted graphs, it's time to create the first module of our temporal set prediction model - a stack of convolutional layers.\n",
    "\n",
    "To quickly recap and slightly formalize what we've done so far: The input for this module is a sequence of dynamic graphs $\\mathbb{G}_i = \\{\\mathcal{G}_i^1,...\\mathcal{G}_i^T\\}$, where graph $\\mathcal{G}_i^t \\in \\mathbb{G}_i$ contains all nodes ever purchased by this household. These are represented as $\\{e_{i,j}^t \\in \\mathbb{R}^{N \\times 1}, \\forall v_{i,j} \\in \\mathcal{V}_i\\}$. ($N \\times 1$ is the dimension of node features ( equal to `in_features` and *i* is the considered household).\n",
    "\n",
    "For each graph $\\mathcal{G}_i$, the output of this convolutional module is a new sequence representation, which we will denote as  $\\{c_{i,j}^t \\in \\mathbb{R}^{F_{1}}, \\forall v_{i,j} \\in \\mathcal{V}_i\\}$. ($F_{1}$ is the new dimension equal to `out_features`). We've set the value of item embeddings already (for naming the stored files so we don't override something), so we can print it out! We'll do the same for the dimensionality of our the hidden layers GCN layers, which we'll look at next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our convolutional module will output 32-dimensional embeddings\n",
      "Our convolutional module will have 2 hidden layers. The dimensionalities are : [32, 32]\n"
     ]
    }
   ],
   "source": [
    "print(f'Our convolutional module will output {f1}-dimensional embeddings')\n",
    "print(f'Our convolutional module will have {len(hidden_dims)} hidden layers. The dimensionalities are : {hidden_dims}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the parameter scale and also make our method flexible to deal with sequences with variable lengths, a parameter sharing strategy is adopted. The weighted convolutions are implemented by propagating information of elements in each dynamic graphs as follows. For graph $\\mathcal{G}_i$\n",
    "$$c_{i,j}^{t,l+1} = \\sigma\\left( b^l + \\sum_{k \\in N_{i,j}^t \\cup \\{j\\}}   A_i^t[j,k] \\cdot \\left( W^t c_{i,k}^{t,l} \\right) \\right),$$ where $A_i^t[j,k]$ represents the item in j-th row and k-th column of matrix $A_i^t$, which is the edge weight of $v_{i,j}$ and $v_{i,k}$ in graph $\\mathcal{G}_i^t$. The superscript index *l* here denotes the convolutional layer in which the representation is located, and $\\sigma$ denotes the chosen non-linearity for our conovlutional layer. In our case, this will be ReLU. <font color=\"red\"> Are we actually doing parameter sharing at the moment? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional layer**\n",
    "We are going to override the `nn.Module` to construct our convolutional layer.\n",
    "For the convolutions, we're going to use the `GCNConv` layer from the PyG library. The convolutions are realized as follows:\n",
    "\n",
    "$$\\mathbf{X}^{\\prime} = \\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
    "\\mathbf{\\hat{D}}^{-1/2} \\mathbf{X} \\mathbf{\\Theta},$$\n",
    "\n",
    "where $\\mathbf{\\hat{A}} = \\mathbf{A + I}$ is the adjacency matrix of a graph with inserted self-loops, and $\\mathbf{\\hat{D}}$ is its diagonal degree matrix.\n",
    "PyG makes the use of convolutions simple by simpy asking us to input the node feature tensor (data.x) of shape `[num_of_nodes, num_of_features]` and the `torch.data.Data` object's `edge_index`, a graph connectivity tensor in `COO` format with shape `[2, num_of_edges]`\n",
    "\n",
    "Here are some other terms needed to understand the following code, where we construct our convolutional module:\n",
    "\n",
    "`nn.ModuleList()` - Holds submodules in a list. <br>\n",
    "`nn.ReLU()` - Applies the rectified linear unit function element-wise: ReLU(x) = max(0,x) <br>\n",
    "`nn.BatchNorm1d` - Applies Batch Normalization over a 2D or 3D input. $y=\\frac{x-E[x]}{\\sqrt{var[x]+\\epsilon}} \\cdot \\gamma + \\beta$, The mean and standard-deviation are calculated per-dimension over the mini-batches and \\gammaγ and \\betaβ are learnable parameter vectors of size C (where C is the input size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weighted_GCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_sizes, out_features):\n",
    "        '''\n",
    "        :param in_features: int, number of input features\n",
    "        :param hidden_sizes: List[int], list of integers of hidden sizes\n",
    "        :param out_features: int, number of output features\n",
    "        '''\n",
    "        super(weighted_GCN, self).__init__()\n",
    "        # we are going to use 3 layers, first graph conv we wrote before, ReLu function and normalization\n",
    "        gcns, relus, bns = nn.ModuleList(), nn.ModuleList(), nn.ModuleList()\n",
    "        \n",
    "        # layers for hidden_size\n",
    "        input_size = in_features\n",
    "        for hidden_size in hidden_sizes:\n",
    "            \n",
    "            # go through all the hidden layers and call all three functions (modules)\n",
    "            gcns.append(GCNConv(in_channels=input_size, \n",
    "                            out_channels=hidden_size,\n",
    "                            improved=False,\n",
    "                            cached=False,\n",
    "                            add_self_loops=False,\n",
    "                            normalize=False,\n",
    "                            bias=False)) \n",
    "            \n",
    "            relus.append(nn.ReLU())\n",
    "            bns.append(nn.BatchNorm1d(hidden_size))\n",
    "            \n",
    "            input_size = hidden_size # next layer start size will be output from one layer before\n",
    "        \n",
    "        # output layer\n",
    "        gcns.append(GCNConv(in_channels=hidden_sizes[-1], \n",
    "                            out_channels=out_features,\n",
    "                            improved=False,\n",
    "                            cached=False,\n",
    "                            add_self_loops=False,\n",
    "                            normalize=False,\n",
    "                            bias=False\n",
    "                            )\n",
    "                   )\n",
    "        relus.append(nn.ReLU())\n",
    "        bns.append(nn.BatchNorm1d(out_features))\n",
    "        self.gcns, self.relus, self.bns = gcns, relus, bns\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        h = x\n",
    "        for gcn, relu, bn in zip(self.gcns, self.relus, self.bns):\n",
    "            \n",
    "            h = gcn(h, adj_t)\n",
    "            h = bn(h.transpose(1, -1)).transpose(1, -1)\n",
    "            h = relu(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For easier understanding of the the process, we will follow the transformation that one input makes throughout it's journey through our model. Let this be the purchasing habits of household `\"1000\"`. Feel free to toy around and change the index of the chosen household in the following cell, if you want to follow a different household (eg. try \"1001\" or \"1002\")!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CHOSEN_HH = \"1000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Run the following cell to learn more about the houshold we'll be looking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chosen household (1000) made 11 trips to the store in the dataset.\n",
      "We'll be predicting the contents of their 11th basked based on what they bought in the preceding 10.\n",
      "This houshold purchased 88 items in all of its 11 visits to the store.\n",
      "Their final basket contains 25 items.\n"
     ]
    }
   ],
   "source": [
    "baskets = shopping_per_hh[CHOSEN_HH]\n",
    "\n",
    "print(f'The chosen household ({CHOSEN_HH}) made {len(baskets)} trips to the store in the dataset.\\n'\n",
    "      f'We\\'ll be predicting the contents of their {len(baskets)}th basked based on what '\n",
    "      f'they bought in the preceding {len(baskets)-1}.\\n'\n",
    "      f'This houshold purchased {len(list(baskets[0].id.keys()))} items in all of its {11} visits to the store.\\n'\n",
    "      f'Their final basket contains {len(list(baskets[-1].y.to_dense().nonzero()))} items.' )\n",
    "\n",
    "Xbaskets = baskets[:-1]\n",
    "Ybaskets = baskets[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the following cell, we initialize the convolutional module and send the `CHOSEN_HH`'s $T_{max}-1$ baskets through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The newly created list 'convoluted_Xbaskets' is now a <class 'list'> of length 10,where each entry is of size torch.Size([88, 32])\n"
     ]
    }
   ],
   "source": [
    "in_dims = baskets[0].num_features\n",
    "convolutional_model = weighted_GCN(in_dims,\n",
    "                                   hidden_dims,\n",
    "                                   f1)\n",
    "convoluted_Xbaskets = []\n",
    "for basket_graph_at_t in Xbaskets:\n",
    "    with torch.no_grad():\n",
    "        o = convolutional_model(basket_graph_at_t.x, basket_graph_at_t.edge_index)\n",
    "        convoluted_Xbaskets.append(o)\n",
    "\n",
    "print(f'The newly created list \\'convoluted_Xbaskets\\' is now a {type(convoluted_Xbaskets)} of length {len(convoluted_Xbaskets)},'\n",
    "      f'where each entry is of size {convoluted_Xbaskets[0].size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 2: Masked Self Attention\n",
    "Now, we will try to learn temporal dependency among products. As we know from everyday life, some elements (products) will appear in our basket quite frequently and regularly, while others will appear irregularly and occasionally. This makes the temporal dependency learning really hard!\n",
    "\n",
    "Previous models, such as RNNs, fail to model this kind of data, because they do not take into account the temporal dependency learning. We are going to construct our model, using self attention, so that we will not loose this information, we are going to construct a temporal dependeny learning component.\n",
    "\n",
    "As an **INPUT** to this component, we will slightly reshape the sequences we have constructed in the previous step. We will do this to make this masked self-attention module more general. The input sequence will be a 3D tensor of shape `(nodes_num, T_max, features_num)`. Let us first reshape the working example and continue from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reshaped 'convoluted_Xbaskets' is now a <class 'torch.Tensor'> of shape torch.Size([88, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "item_embeddings = {j : [] for j in range(len(convoluted_Xbaskets[0]))}\n",
    "for basket in range(len(convoluted_Xbaskets)):\n",
    "    for item in range(len(convoluted_Xbaskets[basket])):\n",
    "        ## add the embeddings for each item to its corresponding temporal index t in the newly created list\n",
    "        item_embeddings[item].append(convoluted_Xbaskets[basket][item].tolist())\n",
    "\n",
    "convoluted_Xbaskets = torch.tensor(list(item_embeddings.values()))\n",
    "print(f'The reshaped \\'convoluted_Xbaskets\\' is now a {type(convoluted_Xbaskets)} of shape {convoluted_Xbaskets.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The output of the self-attention component will be the sequences of embeddings $\\mathbb{C}_i = \\{C_{i,1},...,C_{i,|\\mathcal{V}_i|}\\} $ where $C_{i,j}=\\{c_{i,j}^1,...,c_{i,j}^T\\}$, are the representations of $v_{i,j}$ over time.\n",
    "\n",
    "<font color=\"red\"> TODO: Tu je Maruša mela našopane enačbe, pa sm jih sam prlepu sm. To sekcijo je treba še mal razdelat. </font>\n",
    "\n",
    "$C_{i,j} \\in \\mathbb{R}^{T \\times F'}\n",
    "\\xrightarrow[\\text{}]{\\text{temporal dependency}}\n",
    "Z_{i,j} \\in \\mathbb{R}^{T \\times F''}$\n",
    "\n",
    "$c_{i,j}^t \\in \\mathcal{R}^{F'}$ <br>\n",
    "$z_{i,j} \\in \\mathcal{R}^{F''}$\n",
    "\n",
    "$Z_{i,j} = softmax\\left( \\frac{(C_{i,j}W_q) \\cdot (C_{i,j}W_k)^T}{\\sqrt{F''}} + M_i \\right) \\cdot (C_{i,j}W_v)$, where $W_q \\in \\mathbb{R}^{F' \\times F''}$, $W_k \\in \\mathbb{R}^{F' \\times F''} $, $W_v \\in \\mathbb{R}^{F' \\times F''}$ are trainable parameters, $Z_{i,j} \\in \\mathbb{R}^{T \\times F''}$ is the stacked representation of $v_{i,j}$'s sequence, $M_i \\in \\mathbb{R}^{T \\times T}$ is a masked matrix, which is used to avoid the future information leakage and guarantee that the state of each timestamp is only affected by its previous states,\n",
    "$$\\begin{equation}\n",
    "  M_i^{t,t'}=\\begin{cases}\n",
    "    0, & \\text{if $t<t'$},\\\\\n",
    "    -\\infty, & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{equation}$$\n",
    "\n",
    "We define the self-attention module and its aggregation module in the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class masked_self_attention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, n_heads=4):\n",
    "        super(masked_self_attention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.per_head_dim = output_dim // n_heads\n",
    "        \n",
    "        # inicialization of the weights as described above in the text\n",
    "        self.Wq = nn.Linear(input_dim, n_heads * self.per_head_dim, bias=False)\n",
    "        self.Wk = nn.Linear(input_dim, n_heads * self.per_head_dim, bias=False)\n",
    "        self.Wv = nn.Linear(input_dim, n_heads * self.per_head_dim, bias=False)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_tensor: tensor, shape (nodes_num, T_max, features_num)\n",
    "        Returns:\n",
    "            output: tensor, shape (nodes_num, T_max, output_dim = features_num)\n",
    "        \"\"\"\n",
    "        \n",
    "        seq_length = input_tensor.shape[1]\n",
    "        # tensor, shape (nodes_num, T_max, n_heads * dim_per_head)\n",
    "        Q = self.Wq(input_tensor)\n",
    "        K = self.Wk(input_tensor)\n",
    "        V = self.Wv(input_tensor)\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Figure out these transposes/reshapes/permutes (and explain/make them prettier if possible)\n",
    "        \"\"\"\n",
    "        \n",
    "        # multi_head attention\n",
    "        # Q, tensor, shape (nodes_num, n_heads, T_max, dim_per_head)\n",
    "        Q = Q.reshape(input_tensor.shape[0], input_tensor.shape[1], self.n_heads, self.per_head_dim).transpose(1, 2)\n",
    "        # K after transpose, tensor, shape (nodes_num, n_heads, dim_per_head, T_max)\n",
    "        K = K.reshape(input_tensor.shape[0], input_tensor.shape[1], self.n_heads, self.per_head_dim).permute(0, 2, 3, 1)\n",
    "        # V, tensor, shape (nodes_num, n_heads, T_max, dim_per_head)\n",
    "        V = V.reshape(input_tensor.shape[0], input_tensor.shape[1], self.n_heads, self.per_head_dim).transpose(1, 2)\n",
    "\n",
    "        # scaled attention_score, tensor, shape (nodes_num, n_heads, T_max, T_max)\n",
    "        attention_score = Q.matmul(K) / np.sqrt(self.per_head_dim)\n",
    "\n",
    "        # attention_mask, tensor, shape -> (T_max, T_max)  -inf in the top and right\n",
    "        attention_mask = torch.zeros(seq_length, seq_length).masked_fill(\n",
    "            torch.tril(torch.ones(seq_length, seq_length)) == 0, -np.inf)\n",
    "\n",
    "        \n",
    "        \n",
    "        # attention_mask will be broadcast to (nodes_num, n_heads, T_max, T_max)\n",
    "        attention_score = attention_score + attention_mask\n",
    "        \n",
    "        \n",
    "        # (nodes_num, n_heads, T_max, T_max)\n",
    "        attention_score = torch.softmax(attention_score, dim=-1)\n",
    "\n",
    "        # multi_result, tensor, shape (nodes_num, n_heads, T_max, dim_per_head)\n",
    "        multi_head_result = attention_score.matmul(V)\n",
    "        # multi_result, tensor, shape (nodes_num, T_max, n_heads * dim_per_head = output_dim)\n",
    "        # concat multi-head attention results\n",
    "        output = multi_head_result.transpose(1, 2).reshape(input_tensor.shape[0],\n",
    "                                                           seq_length, self.n_heads * self.per_head_dim)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class aggregate_nodes_temporal(nn.Module):\n",
    "\n",
    "    def __init__(self, item_embed_dim):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param item_embed_dim: the dimension of input features\n",
    "        \"\"\"\n",
    "        \n",
    "        super(aggregate_nodes_temporal, self).__init__()\n",
    "\n",
    "        self.Wq = nn.Linear(item_embed_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, Z):\n",
    "        ### Equation 4 in the paper\n",
    "        \n",
    "        output = self.Wq(Z).transpose(1,2).matmul(Z).transpose(1,2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's run our rolling `CHOSEN_HH` example through the two modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The preset embedding fim f2 = 32\n",
      "The output 'temporal_Xbaskets' is now a <class 'torch.Tensor'> of shape torch.Size([88, 32])\n"
     ]
    }
   ],
   "source": [
    "print(f'The preset embedding fim f2 = {f2}')\n",
    "model1 = masked_self_attention(input_dim=f1, output_dim=f2)\n",
    "model_2 = aggregate_nodes_temporal(item_embed_dim=f2)\n",
    "h = model1(convoluted_Xbaskets)\n",
    "temporal_Xbaskets = model_2(h)[:,:,0]\n",
    "\n",
    "print(f'The output \\'temporal_Xbaskets\\' is now a {type(temporal_Xbaskets)} of shape {temporal_Xbaskets.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Gated Information Fusing\n",
    "With the previous two components, we've learned the dynamic temporal representations of the baskets. However, we need to fuse this knowledge with our latent fammiliarity with all possible items in the store. For this, we will return to the dictionary `reverse_uid` that we initialized in the beginning.\n",
    "\n",
    "Using it, we'll initialize a matrix $\\textbf{E}$, which could be seen as a static representation of all the elements. It is shared by all households. This is an advantage, as we could use it also for sparse data and we do not need any additional information about the purchasing household.\n",
    "\n",
    "We'll fuse this matrix of dimensionality $(\\hat{N}, F^{''}),$ with the compact temporal element representations w.r.t. to household  $u_i$ we constructed in the previous step.\n",
    "\n",
    "We use $E_i$ to denote the hidden state of household $i$, which is initialized as $E$. The most recent state $E^{update}_{i, I(j)}$ is achieved by updating the household state $E_i$ iteratively as follows:,\n",
    "$$E^{update}_{i, I(j)} = (1- \\beta_{i,I(j)} \\cdot \\gamma_{I(j)}) \\cdot E_{i,I(j)} + (\\beta_{i,I(j)} \\cdot \\gamma_{I(j)}) \\cdot z_{i,j},$$ where $I(\\cdot)$ is a function that maps element $v_{i,j}$ to its corresponding index in $E_i$ (in our case, the convinient `reverse_uid` dictionary) and $\\beta_{i,j}$ and $\\gamma_j$ are the j-th dimention of $\\beta_i$ and $\\gamma$.\n",
    "\n",
    "We define our gated update module in the following cell:\n",
    "<font color=\"red\"> TODO: Check if we can reduce the number of arguments this model's constructor and forward function take. Don't forget to fix all the calls of this function if tweaking. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class global_gated_update(nn.Module):\n",
    "    ### num_all_unique_items, f0\n",
    "    def __init__(self, items_total, f0, item_dict):\n",
    "        super(global_gated_update, self).__init__()\n",
    "        \n",
    "        self.num_items_total = items_total\n",
    "        self.embedding_dim = f0\n",
    "        self.E = torch.randn((self.num_items_total, self.embedding_dim))\n",
    "        self.gamma = nn.Parameter(torch.rand(self.num_items_total, 1), requires_grad=True)\n",
    "        self.item_dict = {int(i):item_dict[i] for i in item_dict}\n",
    "        \n",
    "    def forward(self, ids, Z):\n",
    "        num_nodes = len(ids)\n",
    "        beta = torch.zeros(self.num_items_total, 1)\n",
    "        ### masking\n",
    "        nodes_in_graph = ids\n",
    "        rows_in_E = [self.item_dict[code.item()] for code in nodes_in_graph]\n",
    "\n",
    "        beta[rows_in_E] = 1\n",
    "        ### update\n",
    "        E_clone = self.E.clone()\n",
    "        ei_update = (1 - beta * self.gamma) * E_clone\n",
    "        #embed[output_nodes, :] = embed[output_nodes, :] + self.gamma[output_nodes] * output_node_features\n",
    "        #print(self.gamma[rows_in_E] * Z)\n",
    "        ei_update[rows_in_E, :] = ei_update[rows_in_E, :] + self.gamma[rows_in_E] * Z        \n",
    "        return ei_update       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And now, let's send our `CHOSEN_HH` training baskets through! <font color=\"red\"> TODO: The code gets needleessly ugly here. We need to optimize the notebook so that we don't have to refractor reverse_uid here. Also, we need to figure out how to avoid inputting `ids` as a parameter into this (and subsequently, the master) model. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output 'E_Xbaskets' is now a <class 'torch.Tensor'> of shape torch.Size([88, 32]). It will be used for prediction.\n"
     ]
    }
   ],
   "source": [
    "model_fuse = global_gated_update(num_all_unique_items, f0, reverse_uid)\n",
    "ids = torch.tensor([int(i) for i in list(shopping_per_hh[CHOSEN_HH][0].id.values())])\n",
    "E_Xbaskets = model_fuse(ids, temporal_Xbaskets)\n",
    "print(f'The output \\'E_Xbaskets\\' is now a {type(temporal_Xbaskets)} of shape {temporal_Xbaskets.size()}. It will be used for prediction.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Putting it all together\n",
    "In the cells above, we ran a test example through the modules of our model one by one. Now, we define a *master* class which will include all of the above modules. After computing the representations like the `E_Xbaskets` we saw above, it will simply add a prediction layer `fc_output`, that can be formalized as:\n",
    "$$\\hat{y}_i = sigmoid(E_i^{update} \\cdot w_0 + b_0),$$ where $w_0 \\in \\mathbb{R}^F$ and $b_0 \\in \\mathbb{R}$ are the trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class temporal_set_prediction(nn.Module):\n",
    "    def __init__(self, items_total, item_embedding_dim, hidden_dims, reverse_uid):\n",
    "        \"\"\"\n",
    "        :param items_total: int\n",
    "        :param item_embedding_dim: int\n",
    "        :param n_heads: int\n",
    "        :param attention_aggregate: sre\n",
    "        \"\"\"\n",
    "        super(temporal_set_prediction, self).__init__()\n",
    "\n",
    "        ### To je njegov f0\n",
    "        self.item_embedding_dim = item_embedding_dim\n",
    "        \n",
    "        self.reverse_uid = reverse_uid\n",
    "        ## to je njegov num_all_unique_items\n",
    "        self.items_total = items_total\n",
    "        \n",
    "        self.hidden_dims = hidden_dims\n",
    "        \n",
    "        \n",
    "        self.our_gcn = weighted_GCN(1, self.hidden_dims, self.item_embedding_dim)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.stacked_gcn = stacked_weighted_GCN_blocks([weighted_GCN(item_embedding_dim,\n",
    "                                                                     [item_embedding_dim],\n",
    "                                                                     item_embedding_dim)])\n",
    "        \"\"\"\n",
    "\n",
    "        self.masked_self_attention = masked_self_attention(input_dim=self.item_embedding_dim,\n",
    "                                                           output_dim=self.item_embedding_dim)\n",
    "\n",
    "        self.aggregate_nodes_temporal_feature = aggregate_nodes_temporal(self.item_embedding_dim)\n",
    "\n",
    "\n",
    "        \n",
    "        #\n",
    "        #(num_all_unique_items, f0, reverse_uid\n",
    "        #\n",
    "        self.global_gated_update = global_gated_update(items_total=self.items_total,\n",
    "                                                       f0=self.item_embedding_dim,\n",
    "                                                       item_dict=self.reverse_uid)\n",
    "\n",
    "        self.fc_output = nn.Sequential(nn.Linear(self.item_embedding_dim, 1, bias=True),\n",
    "                                       nn.Sigmoid())\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, graph_list_for_hh, hh_ids):\n",
    "        embeddings_at_t = []\n",
    "        for graph in graph_list_for_hh:\n",
    "            o = self.our_gcn(graph.x,graph.edge_index)\n",
    "            embeddings_at_t.append(o)\n",
    "        item_embeddings = {j : [] for j in range(len(embeddings_at_t[0]))}\n",
    "        for t in range(len(embeddings_at_t)):\n",
    "            for j in range(len(embeddings_at_t[t])):\n",
    "                ## and add the embeddings for each item to its corresponding temporal index t in the newly created list\n",
    "                item_embeddings[j].append(embeddings_at_t[t][j].tolist())\n",
    "\n",
    "        ## convert the final 3D array to a tensor and save it to the dictionary for further use.\n",
    "        h = torch.tensor(list(item_embeddings.values()))\n",
    "\n",
    "        h = self.masked_self_attention(h)\n",
    "        h = self.aggregate_nodes_temporal_feature(h)\n",
    "        h = h[:,:,0]\n",
    "        #ids = torch.tensor([i[0] for i in list(graph_list_for_hh[0].id.values())])\n",
    "        h = self.global_gated_update(hh_ids, h)\n",
    "        out = self.fc_output(h).squeeze(dim=-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's send the original `XBaskets` thorugh the entire model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make sure if our prediction is working, we can compare its shape to the y tensor, stored in Ybaskets:\n",
      " (torch.Size([24375]) == (torch.Size([24375])!\n",
      "\n",
      "Because we're running it through a Sigmoid, the values in the prediction should be in [0, 1]:\n",
      "tensor([0.5494, 0.4540, 0.3943,  ..., 0.3928, 0.2911, 0.1548])\n"
     ]
    }
   ],
   "source": [
    "final_model = temporal_set_prediction(num_all_unique_items, f0, hidden_dims, reverse_uid)\n",
    "with torch.no_grad():\n",
    "    prediction = final_model(Xbaskets, ids)\n",
    "print(f'To make sure if our prediction is working, we can compare its shape to the y tensor, stored in Ybaskets:\\n'\n",
    "      f''\n",
    "      f' ({prediction.size()} == ({Ybaskets.y.to_dense().size()}!\\n')\n",
    "\n",
    "print(f'Because we\\'re running it through a Sigmoid, the values in the prediction should be in [0, 1]:\\n{prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## TRAINING\n",
    "In the following cells we define some utility functions for training and evaluating our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font color=\"red\"> TODO!!! Ta koda spodi je delala, ampak zdej je neki narobe z Loss Functioni. Problem sm opisu na [Stack Overflow](https://stackoverflow.com/questions/70320940/nonsensical-torch-nn-bceloss-runtimeerror-all-elements-of-input-should-be-betwe). Ne vem kako popravit. Need help. Ko se to popravi, bi načeloma moglo delat. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, ids, train_list, test_data, optimizer, loss_fn):\n",
    "    # TODO: Implement a function that trains the model by \n",
    "    # using the given optimizer and loss_fn.\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = model(train_list, ids)\n",
    "    loss = loss_fn(out, test_data.y.to_dense())\n",
    "\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def eval_loss(model, ids, train_list, test_data, loss_fn):\n",
    "    model.eval()\n",
    "    out = model(train_list, ids)\n",
    "    loss = loss_fn(out, test_data.y.to_dense())\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model fitting example, we will select only such households that made at least 5 purchases in the observed period.\n",
    "\n",
    "Since we encode the ground-truths as multi-hot encodings, we can use Binary cross entropy as a good proxy loss for our desired outcome. Henc, we use - `torch.nn.BCELoss`.\n",
    "\n",
    "We will train our model using the Adam optimizer, using the starting learning rate of 1e-4. <font color=\"red\"> TODO: Currently, we're not using any normalization. Add a note on it here if we change this.</font>\n",
    "\n",
    "To boost the number of our training samples, we will employ a sliding window training approach. For each household in the training set, we'll train several times, each time predicting a basket *t* by learning from baskets $\\{1, \\dots, t-1\\}$.\n",
    "\n",
    "For our loss estimation, we employ a 80/20 holdout policy on the level of households. An additional 15% of all training households are reserved for validation, with a different set being picked in each training epoch. We use the handy functions from `sklearn.model_selection` to efficiently perform all of these dataset splits.\n",
    "\n",
    "Finally, we employ an early stopping mechanism, where we require the validation performance to improve over a rolling period of three epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Households: 100%|██████████| 537/537 [02:00<00:00,  4.47it/s]\n",
      "Calculating Validation Loss: 100%|██████████| 95/95 [00:07<00:00, 12.00it/s]\n",
      "Calculating Test Loss: 100%|██████████| 159/159 [00:10<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00, Train: 1304.6354,   Valid: 28.97Test: 47.83  epoch-best_valid_epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Households: 100%|██████████| 537/537 [01:58<00:00,  4.54it/s]\n",
      "Calculating Validation Loss: 100%|██████████| 95/95 [00:07<00:00, 13.16it/s]\n",
      "Calculating Test Loss: 100%|██████████| 159/159 [00:09<00:00, 16.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train: 528.1441,   Valid: 12.85Test: 20.55  epoch-best_valid_epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Households: 100%|██████████| 537/537 [02:00<00:00,  4.46it/s]\n",
      "Calculating Validation Loss: 100%|██████████| 95/95 [00:07<00:00, 12.74it/s]\n",
      "Calculating Test Loss: 100%|██████████| 159/159 [00:10<00:00, 15.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Train: 224.5380,   Valid: 10.03Test: 14.94  epoch-best_valid_epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Households: 100%|██████████| 537/537 [02:01<00:00,  4.43it/s]\n",
      "Calculating Validation Loss: 100%|██████████| 95/95 [00:07<00:00, 12.76it/s]\n",
      "Calculating Test Loss: 100%|██████████| 159/159 [00:10<00:00, 15.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03, Train: 98.4684,   Valid: 4.89Test: 6.34  epoch-best_valid_epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Households: 100%|██████████| 537/537 [01:57<00:00,  4.55it/s]\n",
      "Calculating Validation Loss: 100%|██████████| 95/95 [00:07<00:00, 13.37it/s]\n",
      "Calculating Test Loss: 100%|██████████| 159/159 [00:09<00:00, 16.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04, Train: 44.8838,   Valid: 4.55Test: 5.46  epoch-best_valid_epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Households: 100%|██████████| 537/537 [01:58<00:00,  4.53it/s]\n",
      "Calculating Validation Loss: 100%|██████████| 95/95 [00:07<00:00, 12.50it/s]\n",
      "Calculating Test Loss: 100%|██████████| 159/159 [00:10<00:00, 15.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Train: 21.9314,   Valid: 5.47Test: 6.99  epoch-best_valid_epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Households: 100%|██████████| 537/537 [01:58<00:00,  4.54it/s]\n",
      "Calculating Validation Loss: 100%|██████████| 95/95 [00:07<00:00, 12.12it/s]\n",
      "Calculating Test Loss: 100%|██████████| 159/159 [00:10<00:00, 15.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06, Train: 12.1328,   Valid: 5.79Test: 7.92  epoch-best_valid_epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Households: 100%|██████████| 537/537 [01:58<00:00,  4.53it/s]\n",
      "Calculating Validation Loss: 100%|██████████| 95/95 [00:07<00:00, 12.56it/s]\n",
      "Calculating Test Loss: 100%|██████████| 159/159 [00:10<00:00, 15.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07, Train: 8.0659,   Valid: 19.08Test: 27.18  epoch-best_valid_epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Households: 100%|██████████| 537/537 [02:01<00:00,  4.40it/s]\n",
      "Calculating Validation Loss: 100%|██████████| 95/95 [00:07<00:00, 13.45it/s]\n",
      "Calculating Test Loss: 100%|██████████| 159/159 [00:09<00:00, 16.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08, Train: 6.4618,   Valid: 17.24Test: 24.88  epoch-best_valid_epoch: 4\n",
      "Early Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "import copy\n",
    "from torchmetrics import Recall\n",
    "import statistics as st\n",
    "f0 = 32\n",
    "\n",
    "\n",
    "model = temporal_set_prediction(num_all_unique_items, f0, hidden_dims, reverse_uid)\n",
    "\n",
    "## make the dictionary into a list that we'll be train-test splitting over\n",
    "list_of_dats = [shopping_per_hh[hh] for hh in shopping_per_hh]\n",
    "\n",
    "### filter the list so that we only have households with at least 5 observations\n",
    "at_least_5 = [i for i in list_of_dats if len(i)>=5]\n",
    "\n",
    "## get the (joint) training data and the test data.\n",
    "training_data, test_idx = train_test_split(\n",
    "    at_least_5, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "## Put the test data in its data loader.\n",
    "test_loader = DataLoader(test_idx)\n",
    "\n",
    "### initialize the loss, evaluators, optimizer and rolling window splitter\n",
    "### that we're gonna us during training.\n",
    "loss_fn = nn.BCELoss()\n",
    "KfortopK = 10\n",
    "rolling_window_splitter = TimeSeriesSplit(test_size=1, gap=0, n_splits = 4)\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "\n",
    "## specify the number of epochs to train for. In our case, early stopping comes into account much earlier.\n",
    "n_epochs = 50\n",
    "\n",
    "best_model = None\n",
    "best_valid_loss= np.inf\n",
    "best_valid_epoch = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    num_skipped = 0\n",
    "    ## On each iteration, we change which data we use for validation.\n",
    "    ## Then, wee put them into loaders\n",
    "    train_idx, valid_idx= train_test_split(\n",
    "        training_data, test_size=0.15, random_state=200, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    tl = DataLoader(train_idx)\n",
    "    vl = DataLoader(valid_idx)\n",
    "    loss = 0\n",
    "\n",
    "\n",
    "    for batch in tqdm(tl, desc='Training Households'):\n",
    "        # we get 4 splits from our household (we learn from each household multiple times).\n",
    "        ids = torch.tensor([int(i[0]) for i in list(batch[-1].id.values())])\n",
    "        loss += train(model, ids, batch[:-1], batch[-1], optimizer, loss_fn)\n",
    "        for learn_from, target_in_list in rolling_window_splitter.split(batch):\n",
    "\n",
    "                target = batch[target_in_list[0]]\n",
    "                lrnd = [batch[i] for i in learn_from]\n",
    "                loss += train(model, ids, lrnd, target, optimizer, loss_fn)\n",
    "\n",
    "\n",
    "\n",
    "    valid_loss = 0\n",
    "    test_loss = 0\n",
    "    for batch in tqdm(vl, desc='Calculating Validation Loss'):\n",
    "        ids = torch.tensor([int(i[0]) for i in list(batch[-1].id.values())])\n",
    "        valid_loss += eval_loss(model, ids, batch[:-1], batch[-1], loss_fn)\n",
    "\n",
    "\n",
    "\n",
    "    for batch in tqdm(test_loader, desc='Calculating Test Loss'):        # we get 4 splits from our household (we learn from each household multiple times).\n",
    "        ids = torch.tensor([int(i[0]) for i in list(batch[-1].id.values())])\n",
    "        test_loss += eval_loss(model, ids, batch[:-1], batch[-1], loss_fn)\n",
    "\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_valid_epoch = epoch\n",
    "\n",
    "    # todo - this loss isn't the actual loss.\n",
    "    print(f'Epoch: {epoch:02d}, '\n",
    "      f'Train: {loss:.4f},   '\n",
    "      f'Valid: {valid_loss:.2f}'\n",
    "      f'Test: {test_loss:.2f} '\n",
    "          f' epoch-best_valid_epoch: {epoch-best_valid_epoch}')\n",
    "\n",
    "    if epoch-best_valid_epoch > 3:\n",
    "        print('Early Stopping.')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further evaluation, let's select the model that performed best on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## EVALUATION\n",
    "We've seen that the model's performance was improving on the heldout testing set on each training epoch already. However, BCE is not a very interpretable loss. We need to <font color=\"red\"> Tu mi ni uspelo nadaljevat</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from statistics import mean, stdev\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def RatK(model, test_loader, show_plot=False):\n",
    "    RaK = {i : [] for i in range(10, 100, 10)}\n",
    "    for batch in tqdm(test_loader, desc='testing'):\n",
    "        ids = torch.tensor([int(i[0]) for i in list(batch[-1].id.values())])\n",
    "        train_list = batch[:-1]\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(train_list, ids)\n",
    "        for K in range(10,100,10):\n",
    "            RaK[K].append(sum(el in torch.argsort(out, descending=True)[:K] for el in batch[-1].y.to_dense().nonzero())/len(batch[-1].y.to_dense().nonzero()))\n",
    "\n",
    "\n",
    "    plot = {'K' : [], 'RatK' : [], 'plus' : [], 'minus' : [], 'stdev' : []}\n",
    "    for K in RaK:\n",
    "        plot['K'].append(K)\n",
    "        plot['RatK'].append(mean(RaK[K]))\n",
    "        plot['plus'].append(mean(RaK[K])+stdev(RaK[K]))\n",
    "        plot['minus'].append(mean(RaK[K])-stdev(RaK[K]))\n",
    "        plot['stdev'].append(stdev(RaK[K]))\n",
    "\n",
    "    RaKdf = pd.DataFrame(RaK)\n",
    "    plotdf = pd.DataFrame(plot)\n",
    "    melted = RaKdf.melt()\n",
    "\n",
    "    plt.style.use('seaborn')\n",
    "    size = (15, 5)\n",
    "    fig, ax = plt.subplots(1,2, figsize=size)\n",
    "    sns.barplot(data=melted, x='variable', y='value', ax = ax[0])\n",
    "    ax[0].set_xlabel('K')\n",
    "    ax[0].set_ylabel('Recall @ K')\n",
    "    ax[0].set_ylim(0, 1)\n",
    "    sns.lineplot(data=plotdf, x='K', y='RatK', ax=ax[1], alpha=1, ls='-', color='blue')\n",
    "    sns.lineplot(data=plotdf, x='K', y='plus', ax=ax[1], alpha=0.8, ls='--', color='blue')\n",
    "    sns.lineplot(data=plotdf, x='K', y='minus', ax=ax[1], alpha=0.8, ls='--', color='blue')\n",
    "    ax[1].set_ylabel('Recall@K')\n",
    "    ax[1].set_ylim(0, 1)\n",
    "    plt.suptitle('Recall@K for different values of K', fontsize=20)\n",
    "    if show_plot:\n",
    "        fig.show()\n",
    "    return {'df' : plotdf, 'fig' : fig}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing: 100%|██████████| 159/159 [00:44<00:00,  3.61it/s]\n",
      "C:\\Users\\vs6309\\AppData\\Local\\Temp/ipykernel_16260/936795506.py:44: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = RatK(model, test_loader, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLG-Project",
   "language": "python",
   "name": "mlg-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
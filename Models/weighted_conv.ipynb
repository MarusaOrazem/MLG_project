{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEIGHTED CONVOLUTION ON DYNAMIC GRAPHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch_sparse/_convert_cpu.so, 6): Symbol not found: __ZN2at5emptyEN3c108ArrayRefIxEENS0_13TensorOptionsENS0_8optionalINS0_12MemoryFormatEEE\n  Referenced from: /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch_sparse/_convert_cpu.so\n  Expected in: /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/lib/libtorch_cpu.dylib\n in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch_sparse/_convert_cpu.so",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b4980df3e0bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch_geometric/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_debug_enabled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch_geometric/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhetero_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHeteroData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtemporal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemporalData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch_geometric/data/data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m from typing import (Optional, Dict, Any, Union, List, Iterable, Tuple,\n\u001b[1;32m      2\u001b[0m                     NamedTuple, Callable)\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch_geometric/typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Types for accessing data ####################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch_sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m ]:\n\u001b[1;32m     15\u001b[0m     torch.ops.load_library(importlib.machinery.PathFinder().find_spec(\n\u001b[0;32m---> 16\u001b[0;31m         f'{library}_{suffix}', [osp.dirname(__file__)]).origin)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/_ops.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;31m# static (global) initialization code in order to register custom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;31m# operators with the JIT.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded_libraries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch_sparse/_convert_cpu.so, 6): Symbol not found: __ZN2at5emptyEN3c108ArrayRefIxEENS0_13TensorOptionsENS0_8optionalINS0_12MemoryFormatEEE\n  Referenced from: /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch_sparse/_convert_cpu.so\n  Expected in: /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/lib/libtorch_cpu.dylib\n in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch_sparse/_convert_cpu.so"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to construct a convolutions on dynamic graphs. \n",
    "Input for this module is a sequence of dynamic graphs $\\mathbb{G}_i = \\{\\mathcal{G}_i^1,...\\mathcal{G}_i^T\\}$, where graph $\\mathcal{G}_i^t \\in \\mathbb{G}_i$ has a sequene of elements represented as $\\{e_{i,j}^t \\in \\mathbb{R}^F, \\forall v_{i,j} \\in \\mathcal{V}_i\\}$. (F is the dimention of element representation equal to `in_features` and *i* is the considered household)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each graph $\\mathcal{G}_i$ the output of this modelue is a new sequence representation, which we will denote as  $\\{c_{i,j}^t \\in \\mathbb{R}^{F'}, \\forall v_{i,j} \\in \\mathcal{V}_i\\}$. (F' is the new dimension equal to `out_features`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the parameter scale and also make our method flexible to deal with sequences with variable lengths, a parameter sharing strategy is adopted. The weighted convolutions are implemented by propagating information of elements in each dynamic graphs as follows. For graph $\\mathcal{G}_i$\n",
    "$$c_{i,j}^{t,l+1} = \\sigma\\left( b^l + \\sum_{k \\in N_{i,j}^t \\cup \\{j\\}}   A_i^t[j,k] \\cdot \\left( W^t c_{i,k}^{t,l} \\right) \\right),$$ where $A_i^t[j,k]$ represents the item in j-th row and k-th column of matrix $A_i^t$, which is the edge weight of $v_{i,j}$ and $v_{i,k}$ in graph $\\mathcal{G}_i^t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to override the `nn.Module` for constructing our convolutional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional layer**\n",
    "For the convolutions, we're going to use the `GCNConv` layer from the PyG library. The convolutions are realized as follows:\n",
    "\n",
    "$$\\mathbf{X}^{\\prime} = \\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
    "\\mathbf{\\hat{D}}^{-1/2} \\mathbf{X} \\mathbf{\\Theta}$$, where $\\mathbf{\\hat{A}} = \\mathbf{A + I}$ is the adjacency matrix of a graph with inserted self-loops, and $\\mathbf{\\hat{D}}$ is its diagonal degree matrix.\n",
    "\n",
    "PyG makes the use of convolutions simple by simpy asking us to input the node feature tensor of shape `[num_of_nodes, num_of_features]` and its Sparse transposed adjecency matrix `adj_t`, which takes into account the weights in our graphs.\n",
    "\n",
    "Here are some other terms needed to understand the following code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.ModuleList()` - Holds submodules in a list. <br>\n",
    "`nn.ReLU()` - Applies the rectified linear unit function element-wise: ReLU(x) = max(0,x) <br>\n",
    "`nn.BatchNorm1d` - Applies Batch Normalization over a 2D or 3D input. $y=\\frac{x-E[x]}{\\sqrt{var[x]+\\epsilon}} \\cdot \\gamma + \\beta$, The mean and standard-deviation are calculated per-dimension over the mini-batches and \\gammaγ and \\betaβ are learnable parameter vectors of size C (where C is the input size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weighted_GCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_sizes, out_features):\n",
    "        '''\n",
    "        :param in_features: int, number of input features\n",
    "        :param hidden_sizes: List[int], list of integers of hidden sizes\n",
    "        :param out_features: int, number of output features\n",
    "        '''\n",
    "        super(weighted_GCN, self).__init__()\n",
    "        # we are going to use 3 layers, first graph conv we wrote before, ReLu function and normalization\n",
    "        gcns, relus, bns = nn.ModuleList(), nn.ModuleList(), nn.ModuleList()\n",
    "        \n",
    "        # layers for hidden_size\n",
    "        input_size = in_features\n",
    "        for hidden_size in hidden_sizes:\n",
    "            # go through all the layers and call all three functions\n",
    "            gcns.append(GCNConv(in_channels=input_size, \n",
    "                            out_channels=hidden_size,\n",
    "                            improved=False,\n",
    "                            cached=False,\n",
    "                            add_self_loops=False,\n",
    "                            normalize=False,\n",
    "                            bias=False)) \n",
    "            relus.append(nn.ReLU())\n",
    "            bns.append(nn.BatchNorm1d(hidden_size))\n",
    "            input_size = hidden_size # next layer start size will be output from one layer before\n",
    "        \n",
    "        # output layer\n",
    "        gcns.append(GCNConv(in_channels=hidden_sizes[-1], \n",
    "                            out_channels=out_features,\n",
    "                            improved=False,\n",
    "                            cached=False,\n",
    "                            add_self_loops=False,\n",
    "                            normalize=False,\n",
    "                            bias=False\n",
    "                            )\n",
    "                   )\n",
    "        relus.append(nn.ReLU())\n",
    "        bns.append(nn.BatchNorm1d(out_features))\n",
    "        self.gcns, self.relus, self.bns = gcns, relus, bns\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        \"\"\"\n",
    "        :param graph: dgl.DGLGraph\n",
    "        :param node_features: torch.Tensor shape (n_1+n_2+..., n_features)\n",
    "               edges_weight: torch.Tensor shape (T, n_1^2+n_2^2+...)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        h = x\n",
    "        for gcn, relu, bn in zip(self.gcns, self.relus, self.bns):\n",
    "            \n",
    "            #run the Convolutional layer\n",
    "            h = gcn(h, adj_t)\n",
    "            #run the batch norm\n",
    "            h = bn(h.transpose(1, -1)).transpose(1, -1)\n",
    "            #run the ReLu\n",
    "            h = relu(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graphs from files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import os.path as osp\n",
    "import networkx as nx\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import erdos_renyi_graph, to_networkx, from_networkx\n",
    "import torch_geometric.transforms as T\n",
    "import torch_sparse\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "f1 = 5  ## F'\n",
    "hidden_dims = [256, 256]\n",
    "\n",
    "shopping_per_hh = {}\n",
    "\n",
    "#This is just a test -- we're only constructing the graphs for houshold id 22 as a proof of concept!\n",
    "print('Creating graphs from files')\n",
    "if not path.exists(path.join(\"..\\\\data\\\\pickles\", f\"shopping_per_hh_F_{f1}.pkl.gz\")):\n",
    "    for filename in tqdm(os.listdir(\"../data/Test-Graphs/content/Graphs/\")):\n",
    "        splits = filename.split('_')\n",
    "        hh_id = splits[0]\n",
    "        if hh_id not in shopping_per_hh: shopping_per_hh[hh_id] = []\n",
    "\n",
    "\n",
    "        ## we construct a NX graph and cast it to pytorch.data.Data\n",
    "        G = nx.Graph(nx.read_pajek(os.path.join(\"../data/Test-Graphs/content/Graphs/\",filename)))\n",
    "        data = from_networkx(G)\n",
    "\n",
    "        ## Then, we override the data.x in data to get the desired format of the dimensions.\n",
    "        ## We're just using a vector of ones here. We can chamge this in the long run to get more expressivness.\n",
    "        x = torch.ones(G.number_of_nodes(), 1)\n",
    "        data.x = x\n",
    "\n",
    "        shopping_per_hh[hh_id].append(data)\n",
    "    \n",
    "    with open(path.join(\"..\\\\data\\\\pickles\", f\"shopping_per_hh_F_{f1}.pkl.gz\"), \"wb\") as f:\n",
    "        pickle.dump(shopping_per_hh, f)\n",
    "\n",
    "else:\n",
    "    with open(path.join(\"..\\\\data\\\\pickles\", f\"shopping_per_hh_F_{f1}.pkl.gz\"), \"rb\") as f:\n",
    "        shopping_per_hh = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running models and converting them to tensors\n"
     ]
    }
   ],
   "source": [
    "final_tensors = {}\n",
    "print('Running models and converting them to tensors')\n",
    "if not path.exists(path.join(\"..\\\\data\\\\pickles\", f\"final_tensors_F_{f1}.pkl.gz\")): \n",
    "    for hh in tqdm(list(shopping_per_hh.keys())):\n",
    "        try:   \n",
    "            in_dims = shopping_per_hh[hh][0].num_features\n",
    "            model = weighted_GCN(in_dims, \n",
    "                                 hidden_dims, \n",
    "                                 f1)\n",
    "\n",
    "            embeddings_at_t = []\n",
    "            ## iterate over all graphs for a givn household\n",
    "            for i in range(len(shopping_per_hh[hh])):\n",
    "                graph = shopping_per_hh[hh][i]\n",
    "                o = model(graph.x,graph.edge_index)\n",
    "                embeddings_at_t.append(o)\n",
    "\n",
    "            ## initialize a dictionary of lists for each item purchased by this household at a shop\n",
    "            item_embeddings = {j : [] for j in range(len(embeddings_at_t[0]))}\n",
    "            for t in range(len(embeddings_at_t)):\n",
    "                for j in range(len(embeddings_at_t[t])):\n",
    "                    ## and add the embeddings for each item to its corresponding temporal index t in the newly created list\n",
    "                    item_embeddings[j].append(embeddings_at_t[t][j].tolist())\n",
    "\n",
    "            ## convert the final 3D array to a tensor and save it to the dictionary for further use.\n",
    "            final_tensors[hh] = torch.tensor(list(item_embeddings.values()))\n",
    "        except ValueError: continue\n",
    "    with open(path.join(\"..\\\\data\\\\pickles\", f\"final_tensors_F_{f1}.pkl.gz\"), \"wb\") as f:\n",
    "        pickle.dump(final_tensors, f)\n",
    "else:\n",
    "    with open(path.join(\"..\\\\data\\\\pickles\", f\"final_tensors_F_{f1}.pkl.gz\"), \"rb\") as f:\n",
    "        final_tensors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class masked_self_attention_origi(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, n_heads=4):\n",
    "        super(masked_self_attention_origi, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.per_head_dim = output_dim // n_heads\n",
    "        # inicialization of the weights as described above in the text\n",
    "        self.Wq = nn.Linear(input_dim, n_heads * self.per_head_dim, bias=False)\n",
    "        self.Wk = nn.Linear(input_dim, n_heads * self.per_head_dim, bias=False)\n",
    "        self.Wv = nn.Linear(input_dim, n_heads * self.per_head_dim, bias=False)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_tensor: tensor, shape (nodes_num, T_max, features_num)\n",
    "        Returns:\n",
    "            output: tensor, shape (nodes_num, T_max, output_dim = features_num)\n",
    "        \"\"\"\n",
    "        \n",
    "        seq_length = input_tensor.shape[1]\n",
    "        # tensor, shape (nodes_num, T_max, n_heads * dim_per_head)\n",
    "        Q = self.Wq(input_tensor)\n",
    "        K = self.Wk(input_tensor)\n",
    "        V = self.Wv(input_tensor)\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Figure out these transposes/reshapes/permutes (and explain/make them prettier if possible)\n",
    "        \"\"\"\n",
    "        \n",
    "        # multi_head attention\n",
    "        # Q, tensor, shape (nodes_num, n_heads, T_max, dim_per_head)\n",
    "        Q = Q.reshape(input_tensor.shape[0], input_tensor.shape[1], self.n_heads, self.per_head_dim).transpose(1, 2)\n",
    "        # K after transpose, tensor, shape (nodes_num, n_heads, dim_per_head, T_max)\n",
    "        K = K.reshape(input_tensor.shape[0], input_tensor.shape[1], self.n_heads, self.per_head_dim).permute(0, 2, 3, 1)\n",
    "        # V, tensor, shape (nodes_num, n_heads, T_max, dim_per_head)\n",
    "        V = V.reshape(input_tensor.shape[0], input_tensor.shape[1], self.n_heads, self.per_head_dim).transpose(1, 2)\n",
    "\n",
    "        # scaled attention_score, tensor, shape (nodes_num, n_heads, T_max, T_max)\n",
    "        attention_score = Q.matmul(K) / np.sqrt(self.per_head_dim)\n",
    "\n",
    "        # attention_mask, tensor, shape -> (T_max, T_max)  -inf in the top and right\n",
    "        attention_mask = torch.zeros(seq_length, seq_length).masked_fill(\n",
    "            torch.tril(torch.ones(seq_length, seq_length)) == 0, -np.inf)\n",
    "        \n",
    "        \n",
    "        # attention_mask will be broadcast to (nodes_num, n_heads, T_max, T_max)\n",
    "        attention_score = attention_score + attention_mask\n",
    "        \n",
    "        \n",
    "        # (nodes_num, n_heads, T_max, T_max)\n",
    "        attention_score = torch.softmax(attention_score, dim=-1)\n",
    "\n",
    "        # multi_result, tensor, shape (nodes_num, n_heads, T_max, dim_per_head)\n",
    "        multi_head_result = attention_score.matmul(V)\n",
    "        # multi_result, tensor, shape (nodes_num, T_max, n_heads * dim_per_head = output_dim)\n",
    "        # concat multi-head attention results\n",
    "        output = multi_head_result.transpose(1, 2).reshape(input_tensor.shape[0],\n",
    "                                                           seq_length, self.n_heads * self.per_head_dim)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class aggregate_nodes_temporal_feature_origi(nn.Module):\n",
    "\n",
    "    def __init__(self, item_embed_dim):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param item_embed_dim: the dimension of input features\n",
    "        \"\"\"\n",
    "        \n",
    "        super(aggregate_nodes_temporal_feature_origi, self).__init__()\n",
    "\n",
    "        self.Wq = nn.Linear(item_embed_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, Z):\n",
    "        ### Equation 4 in the paper\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: CHECK IF THESE TRANSPOSES ARE OK\n",
    "        \n",
    "        \"\"\"\n",
    "        output = self.Wq(Z).transpose(1,2).matmul(Z).transpose(1,2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the self attention masked tensors for every household and pickle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1\n",
    "f2 = 8 ## F''\n",
    "\"\"\"\n",
    "###TODO - check if masked_self_attention_origi is working OK, since it only returns a tensor of dim=f2 if f2%4==0\n",
    "PS: It's not. Must fix this!!!!!\n",
    "\"\"\"\n",
    "\n",
    "attention_tensors = {}\n",
    "if not path.exists(path.join(\"..\\\\data\\\\pickles\", f\"attention_tensors_F1_{f1}_F2_{f2}.pkl.gz\")): \n",
    "    for hh in tqdm(final_tensors.keys()):\n",
    "        tens = final_tensors[hh]\n",
    "        model1 = masked_self_attention_origi(input_dim=f1, output_dim=f2)\n",
    "        o = model1(tens)\n",
    "        model_2 = aggregate_nodes_temporal_feature_origi(item_embed_dim=f2)\n",
    "        o2 = model_2(o)\n",
    "        attention_tensors[hh] = o2[:,:,0]\n",
    "    with open(path.join(\"..\\\\data\\\\pickles\", f\"attention_tensors_F1_{f1}_F2_{f2}.pkl.gz\"), \"wb\") as f:\n",
    "        pickle.dump(attention_tensors, f)\n",
    "else:\n",
    "    with open(path.join(\"..\\\\data\\\\pickles\", f\"attention_tensors_F1_{f1}_F2_{f2}.pkl.gz\"), \"rb\") as f:\n",
    "        attention_tensors = pickle.load(f)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Information Fusing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "### initialize E for every household\n",
    "f0 = f2\n",
    "temp_Es = {}\n",
    "for i in final_tensors.keys():\n",
    "    temp_Es[i] = torch.randn((final_tensors[i].size()[0], f0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here is an attempt of me constructing a PyG dataset. It's not working as intended atm. \n",
    "## TODO or Deprecated methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Dataset\n",
    "class ShoppingDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return  os.listdir(\"../data/Test-Graphs/content/Graphs/\")\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'not_implemented.pt'\n",
    "\n",
    "    def download(self):\n",
    "        # Download to `self.raw_dir`.\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        idx = 0\n",
    "        for i, dat in enumerate(data_list):\n",
    "            torch.save(data, os.path.join(self.processed_dir, f'data_{i}.pt'))\n",
    "#         for raw_path in self.raw_paths:\n",
    "#             # Read data from `raw_path`.\n",
    "#             data = Data(...)\n",
    "\n",
    "#             if self.pre_filter is not None and not self.pre_filter(data):\n",
    "#                 continue\n",
    "\n",
    "#             if self.pre_transform is not None:\n",
    "#                 data = self.pre_transform(data)\n",
    "\n",
    "#             torch.save(data, osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "#             idx += 1\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        return data\n",
    "\n",
    "ds = ShoppingDataset(root='../data/ShoppingDataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: This still isn't used.\n",
    "## This class was used before for prepping our representations for the attention based temporal learning modulee. It's not used anywhere at the moment.\n",
    "class stacked_weighted_GCN_blocks(nn.ModuleList):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(stacked_weighted_GCN_blocks, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, *input):\n",
    "        nodes_feature, edge_weights = input\n",
    "        h = nodes_feature\n",
    "        for module in self:\n",
    "            h = module(h, edge_weights)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A PyG-attempt at masked_Self_attention\n",
    "class masked_self_attention(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, n_heads=4, concat = True):\n",
    "        super(masked_self_attention, self).__init__()\n",
    "        self.concat = concat\n",
    "\n",
    "        # the dimension of each head is dq // n_heads\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.W_x = nn.Linear(input_dim, output_dim)\n",
    "        self.transformer = torch_geometric.nn.TransformerConv(self.input_dim, self.output_dim, self.n_heads, concat = self.concat)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        output = self.transformer(x, edge_index)\n",
    "        output = output.matmul(self.W_x(x))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class aggregate_nodes_temporal_feature(nn.Module):\n",
    "    def __init__(self, item_embed_dim):\n",
    "        super(aggregate_nodes_temporal_feature, self).__init__()\n",
    "\n",
    "        self.Wq = nn.Linear(item_embed_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, Z):\n",
    "        output = nn.Linear(Z).t().matmul(Z).t()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
=======
   "display_name": "MLG-Project",
   "language": "python",
   "name": "mlg-project"
>>>>>>> aa2a96d129e08f8cc16b2cbe93968209b9a65a3d
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

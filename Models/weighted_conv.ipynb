{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEIGHTED CONVOLUTION ON DYNAMIC GRAPHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to construct a convolutions on dynamic graphs. \n",
    "Input for this module is a sequence of dynamic graphs $\\mathbb{G}_i = \\{\\mathcal{G}_i^1,...\\mathcal{G}_i^T\\}$, where graph $\\mathcal{G}_i^t \\in \\mathbb{G}_i$ has a sequene of elements represented as $\\{e_{i,j}^t \\in \\mathbb{R}^F, \\forall v_{i,j} \\in \\mathcal{V}_i\\}$. (F is the dimention of element representation equal to `in_features` and *i* is the considered household)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each graph $\\mathcal{G}_i$ the output of this modelue is a new sequence representation, which we will denote as  $\\{c_{i,j}^t \\in \\mathbb{R}^{F'}, \\forall v_{i,j} \\in \\mathcal{V}_i\\}$. (F' is the new dimension equal to `out_features`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the parameter scale and also make our method flexible to deal with sequences with variable lengths, a parameter sharing strategy is adopted. The weighted convolutions are implemented by propagating information of elements in each dynamic graphs as follows. For graph $\\mathcal{G}_i$\n",
    "$$c_{i,j}^{t,l+1} = \\sigma\\left( b^l + \\sum_{k \\in N_{i,j}^t \\cup \\{j\\}}   A_i^t[j,k] \\cdot \\left( W^t c_{i,k}^{t,l} \\right) \\right),$$ where $A_i^t[j,k]$ represents the item in j-th row and k-th column of matrix $A_i^t$, which is the edge weight of $v_{i,j}$ and $v_{i,k}$ in graph $\\mathcal{G}_i^t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to override the `nn.Module` for constructing our convolutional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional layer**\n",
    "For the convolutions, we're going to use the `GCNConv` layer from the PyG library. The convolutions are realized as follows:\n",
    "\n",
    "$$\\mathbf{X}^{\\prime} = \\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
    "\\mathbf{\\hat{D}}^{-1/2} \\mathbf{X} \\mathbf{\\Theta}$$, where $\\mathbf{\\hat{A}} = \\mathbf{A + I}$ is the adjacency matrix of a graph with inserted self-loops, and $\\mathbf{\\hat{D}}$ is its diagonal degree matrix.\n",
    "\n",
    "PyG makes the use of convolutions simple by simpy asking us to input the node feature tensor of shape `[num_of_nodes, num_of_features]` and its Sparse transposed adjecency matrix `adj_t`, which takes into account the weights in our graphs.\n",
    "\n",
    "Here are some other terms needed to understand the following code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.ModuleList()` - Holds submodules in a list. <br>\n",
    "`nn.ReLU()` - Applies the rectified linear unit function element-wise: ReLU(x) = max(0,x) <br>\n",
    "`nn.BatchNorm1d` - Applies Batch Normalization over a 2D or 3D input. $y=\\frac{x-E[x]}{\\sqrt{var[x]+\\epsilon}} \\cdot \\gamma + \\beta$, The mean and standard-deviation are calculated per-dimension over the mini-batches and \\gammaγ and \\betaβ are learnable parameter vectors of size C (where C is the input size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weighted_GCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_sizes, out_features):\n",
    "        '''\n",
    "        :param in_features: int, number of input features\n",
    "        :param hidden_sizes: List[int], list of integers of hidden sizes\n",
    "        :param out_features: int, number of output features\n",
    "        '''\n",
    "        super(weighted_GCN, self).__init__()\n",
    "        # we are going to use 3 layers, first graph conv we wrote before, ReLu function and normalization\n",
    "        gcns, relus, bns = nn.ModuleList(), nn.ModuleList(), nn.ModuleList()\n",
    "        \n",
    "        # layers for hidden_size\n",
    "        input_size = in_features\n",
    "        for hidden_size in hidden_sizes:\n",
    "            # go through all the layers and call all three functions\n",
    "            gcns.append(GCNConv(in_channels=input_size, \n",
    "                            out_channels=hidden_size,\n",
    "                            improved=False,\n",
    "                            cached=False,\n",
    "                            add_self_loops=False,\n",
    "                            normalize=False,\n",
    "                            bias=False)) \n",
    "            relus.append(nn.ReLU())\n",
    "            bns.append(nn.BatchNorm1d(hidden_size))\n",
    "            input_size = hidden_size # next layer start size will be output from one layer before\n",
    "        \n",
    "        # output layer\n",
    "        gcns.append(GCNConv(in_channels=hidden_sizes[-1], \n",
    "                            out_channels=out_features,\n",
    "                            improved=False,\n",
    "                            cached=False,\n",
    "                            add_self_loops=False,\n",
    "                            normalize=False,\n",
    "                            bias=False\n",
    "                            )\n",
    "                   )\n",
    "        relus.append(nn.ReLU())\n",
    "        bns.append(nn.BatchNorm1d(out_features))\n",
    "        self.gcns, self.relus, self.bns = gcns, relus, bns\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        \"\"\"\n",
    "        :param graph: dgl.DGLGraph\n",
    "        :param node_features: torch.Tensor shape (n_1+n_2+..., n_features)\n",
    "               edges_weight: torch.Tensor shape (T, n_1^2+n_2^2+...)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        h = x\n",
    "        for gcn, relu, bn in zip(self.gcns, self.relus, self.bns):\n",
    "            \n",
    "            #run the Convolutional layer\n",
    "            h = gcn(h, adj_t)\n",
    "            #run the batch norm\n",
    "            h = bn(h.transpose(1, -1)).transpose(1, -1)\n",
    "            #run the ReLu\n",
    "            h = relu(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graphs from files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import os.path as osp\n",
    "import networkx as nx\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import erdos_renyi_graph, to_networkx, from_networkx\n",
    "import torch_geometric.transforms as T\n",
    "import torch_sparse\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "f1 = 30  ## F'\n",
    "hidden_dims = [256, 256]\n",
    "\n",
    "shopping_per_hh = {}\n",
    "\n",
    "#This is just a test -- we're only constructing the graphs for houshold id 22 as a proof of concept!\n",
    "print('Creating graphs from files')\n",
    "if not path.exists(path.join(\"..\\\\data\\\\pickles\", f\"shopping_per_hh_F1_{f1}_hid_{hidden_dims}.pkl.gz\")):\n",
    "    for filename in tqdm(os.listdir(\"../data/Test-Graphs/content/Graphs/\")):\n",
    "        splits = filename.split('_')\n",
    "        hh_id = splits[0]\n",
    "        if hh_id not in shopping_per_hh: shopping_per_hh[hh_id] = []\n",
    "\n",
    "\n",
    "        ## we construct a NX graph and cast it to pytorch.data.Data\n",
    "        G = nx.Graph(nx.read_pajek(os.path.join(\"../data/Test-Graphs/content/Graphs/\",filename)))\n",
    "\n",
    "        data = from_networkx(G)\n",
    "\n",
    "        ## Then, we override the data.x in data to get the desired format of the dimensions.\n",
    "        ## We're just using a vector of ones here. We can chamge this in the long run to get more expressivness.\n",
    "        x = torch.ones(G.number_of_nodes(), 1)\n",
    "        data.x = x\n",
    "        data.id = {i:code for (i, code) in zip ([i for i in range(G.number_of_nodes())], list(G.nodes()))}\n",
    "        shopping_per_hh[hh_id].append(data)\n",
    "    \n",
    "    with open(path.join(\"..\\\\data\\\\pickles\", f\"shopping_per_hh_F1_{f1}_hid_{hidden_dims}.pkl.gz\"), \"wb\") as f:\n",
    "        pickle.dump(shopping_per_hh, f)\n",
    "\n",
    "else:\n",
    "    with open(path.join(\"..\\\\data\\\\pickles\", f\"shopping_per_hh_F1_{f1}_hid_{hidden_dims}.pkl.gz\"), \"rb\") as f:\n",
    "        shopping_per_hh = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running models and converting them to tensors\n"
     ]
    }
   ],
   "source": [
    "final_tensors = {}\n",
    "print('Running models and converting them to tensors')\n",
    "if not path.exists(path.join(\"..\\\\data\\\\pickles\", f\"final_tensors_F1_{f1}_hid_{hidden_dims}.pkl.gz\")): \n",
    "    for hh in tqdm(list(shopping_per_hh.keys())):\n",
    "        \n",
    "        \"\"\" Check why we need this try/catch block -- if it causes problems, come here and try to fix it.\"\"\"\n",
    "        \n",
    "        try:   \n",
    "            in_dims = shopping_per_hh[hh][0].num_features\n",
    "            model = weighted_GCN(in_dims, \n",
    "                                 hidden_dims, \n",
    "                                 f1)\n",
    "\n",
    "            embeddings_at_t = []\n",
    "            ## iterate over all graphs for a givn household\n",
    "            for i in range(len(shopping_per_hh[hh])):\n",
    "                graph = shopping_per_hh[hh][i]\n",
    "                o = model(graph.x,graph.edge_index)\n",
    "                embeddings_at_t.append(o)\n",
    "\n",
    "            ## initialize a dictionary of lists for each item purchased by this household at a shop\n",
    "            item_embeddings = {j : [] for j in range(len(embeddings_at_t[0]))}\n",
    "            for t in range(len(embeddings_at_t)):\n",
    "                for j in range(len(embeddings_at_t[t])):\n",
    "                    ## and add the embeddings for each item to its corresponding temporal index t in the newly created list\n",
    "                    item_embeddings[j].append(embeddings_at_t[t][j].tolist())\n",
    "\n",
    "            ## convert the final 3D array to a tensor and save it to the dictionary for further use.\n",
    "            final_tensors[hh] = torch.tensor(list(item_embeddings.values()))\n",
    "        except ValueError: continue\n",
    "    with open(path.join(\"..\\\\data\\\\pickles\", f\"final_tensors_F1_{f1}_hid_{hidden_dims}.pkl.gz\"), \"wb\") as f:\n",
    "        pickle.dump(final_tensors, f)\n",
    "else:\n",
    "    with open(path.join(\"..\\\\data\\\\pickles\", f\"final_tensors_F1_{f1}_hid_{hidden_dims}.pkl.gz\"), \"rb\") as f:\n",
    "        final_tensors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class masked_self_attention_origi(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, n_heads=4):\n",
    "        super(masked_self_attention_origi, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.per_head_dim = output_dim // n_heads\n",
    "        # inicialization of the weights as described above in the text\n",
    "        self.Wq = nn.Linear(input_dim, n_heads * self.per_head_dim, bias=False)\n",
    "        self.Wk = nn.Linear(input_dim, n_heads * self.per_head_dim, bias=False)\n",
    "        self.Wv = nn.Linear(input_dim, n_heads * self.per_head_dim, bias=False)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_tensor: tensor, shape (nodes_num, T_max, features_num)\n",
    "        Returns:\n",
    "            output: tensor, shape (nodes_num, T_max, output_dim = features_num)\n",
    "        \"\"\"\n",
    "        \n",
    "        seq_length = input_tensor.shape[1]\n",
    "        # tensor, shape (nodes_num, T_max, n_heads * dim_per_head)\n",
    "        Q = self.Wq(input_tensor)\n",
    "        K = self.Wk(input_tensor)\n",
    "        V = self.Wv(input_tensor)\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Figure out these transposes/reshapes/permutes (and explain/make them prettier if possible)\n",
    "        \"\"\"\n",
    "        \n",
    "        # multi_head attention\n",
    "        # Q, tensor, shape (nodes_num, n_heads, T_max, dim_per_head)\n",
    "        Q = Q.reshape(input_tensor.shape[0], input_tensor.shape[1], self.n_heads, self.per_head_dim).transpose(1, 2)\n",
    "        # K after transpose, tensor, shape (nodes_num, n_heads, dim_per_head, T_max)\n",
    "        K = K.reshape(input_tensor.shape[0], input_tensor.shape[1], self.n_heads, self.per_head_dim).permute(0, 2, 3, 1)\n",
    "        # V, tensor, shape (nodes_num, n_heads, T_max, dim_per_head)\n",
    "        V = V.reshape(input_tensor.shape[0], input_tensor.shape[1], self.n_heads, self.per_head_dim).transpose(1, 2)\n",
    "\n",
    "        # scaled attention_score, tensor, shape (nodes_num, n_heads, T_max, T_max)\n",
    "        attention_score = Q.matmul(K) / np.sqrt(self.per_head_dim)\n",
    "\n",
    "        # attention_mask, tensor, shape -> (T_max, T_max)  -inf in the top and right\n",
    "        attention_mask = torch.zeros(seq_length, seq_length).masked_fill(\n",
    "            torch.tril(torch.ones(seq_length, seq_length)) == 0, -np.inf)\n",
    "        \n",
    "        \n",
    "        # attention_mask will be broadcast to (nodes_num, n_heads, T_max, T_max)\n",
    "        attention_score = attention_score + attention_mask\n",
    "        \n",
    "        \n",
    "        # (nodes_num, n_heads, T_max, T_max)\n",
    "        attention_score = torch.softmax(attention_score, dim=-1)\n",
    "\n",
    "        # multi_result, tensor, shape (nodes_num, n_heads, T_max, dim_per_head)\n",
    "        multi_head_result = attention_score.matmul(V)\n",
    "        # multi_result, tensor, shape (nodes_num, T_max, n_heads * dim_per_head = output_dim)\n",
    "        # concat multi-head attention results\n",
    "        output = multi_head_result.transpose(1, 2).reshape(input_tensor.shape[0],\n",
    "                                                           seq_length, self.n_heads * self.per_head_dim)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class aggregate_nodes_temporal_feature_origi(nn.Module):\n",
    "\n",
    "    def __init__(self, item_embed_dim):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param item_embed_dim: the dimension of input features\n",
    "        \"\"\"\n",
    "        \n",
    "        super(aggregate_nodes_temporal_feature_origi, self).__init__()\n",
    "\n",
    "        self.Wq = nn.Linear(item_embed_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, Z):\n",
    "        ### Equation 4 in the paper\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: CHECK IF THESE TRANSPOSES ARE OK\n",
    "        \n",
    "        \"\"\"\n",
    "        output = self.Wq(Z).transpose(1,2).matmul(Z).transpose(1,2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the self attention masked tensors for every household and pickle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1\n",
    "f2 = 32 ## F''\n",
    "\"\"\"\n",
    "###TODO - check if masked_self_attention_origi is working OK, since it only returns a tensor of dim=f2 if f2%4==0\n",
    "PS: It's not. Must fix this!!!!!\n",
    "\"\"\"\n",
    "\n",
    "attention_tensors = {}\n",
    "if not path.exists(path.join(\"..\\\\data\\\\pickles\", f\"attention_tensors_F1_{f1}_F2_{f2}.pkl.gz\")): \n",
    "    for hh in tqdm(final_tensors.keys()):\n",
    "        tens = final_tensors[hh]\n",
    "        model1 = masked_self_attention_origi(input_dim=f1, output_dim=f2)\n",
    "        o = model1(tens)\n",
    "        model_2 = aggregate_nodes_temporal_feature_origi(item_embed_dim=f2)\n",
    "        o2 = model_2(o)\n",
    "        attention_tensors[hh] = o2[:,:,0]\n",
    "    with open(path.join(\"..\\\\data\\\\pickles\", f\"attention_tensors_F1_{f1}_F2_{f2}.pkl.gz\"), \"wb\") as f:\n",
    "        pickle.dump(attention_tensors, f)\n",
    "else:\n",
    "    with open(path.join(\"..\\\\data\\\\pickles\", f\"attention_tensors_F1_{f1}_F2_{f2}.pkl.gz\"), \"rb\") as f:\n",
    "        attention_tensors = pickle.load(f)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Information Fusing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "### initialize E for every household\n",
    "f0 = f2\n",
    "\n",
    "## change this to the final data source\n",
    "all_unique_items = list(pd.read_csv(os.path.join('..\\\\data\\\\', 'transaction_data_smaller.csv')).PRODUCT_ID.unique())\n",
    "num_all_unique_items = len(all_unique_items)\n",
    "\n",
    "\n",
    "#unique_item_dict = {idx : item_code for idx, item_code in enumerate(all_unique_items)}\n",
    "reverse_uid = {str(item_code) : idx for idx, item_code in enumerate(all_unique_items)}\n",
    "#attention_tensors[\"1000\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class global_gated_update(nn.Module):\n",
    "    ### num_all_unique_items, f0\n",
    "    def __init__(self, items_total, f0, item_dict):\n",
    "        super(global_gated_update, self).__init__()\n",
    "        \n",
    "        self.num_items_total = items_total\n",
    "        self.embedding_dim = f0\n",
    "        self.E = torch.randn((self.num_items_total, self.embedding_dim))\n",
    "        self.gamma = nn.Parameter(torch.rand(self.num_items_total, 1), requires_grad=True)\n",
    "        self.item_dict = item_dict\n",
    "        \n",
    "    def forward(self, ids, Z, f2):\n",
    "        num_nodes = len(ids)\n",
    "        beta = torch.zeros(self.num_items_total, 1)\n",
    "        ### masking\n",
    "        nodes_in_graph = ids\n",
    "        #print(f'nodes in graph = {len(nodes_in_graph)}')\n",
    "        rows_in_E = [self.item_dict[code] for code in nodes_in_graph]\n",
    "        #print(f'rows in E = {len(rows_in_E)}')\n",
    "        beta[rows_in_E] = 1\n",
    "        ### update\n",
    "        ei_update = (1 - beta * self.gamma) * E.clone()\n",
    "        #embed[output_nodes, :] = embed[output_nodes, :] + self.gamma[output_nodes] * output_node_features\n",
    "        #print(self.gamma[rows_in_E] * Z)\n",
    "        ei_update[rows_in_E, :] = ei_update[rows_in_E, :] + self.gamma[rows_in_E] * Z        \n",
    "        return ei_update       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This might not work if the initial graphs don't have identical orderings (.ids) \n",
    "> <font color=\"red\">Yes it does, everything is OK :)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fuse = global_gated_update(num_all_unique_items, f0, reverse_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_updates = {}\n",
    "if not path.exists(path.join(\"..\\\\data\\\\pickles\", f\"E_updates_F1_{f1}_F2_{f2}.pkl.gz\")): \n",
    "    for hh in tqdm(attention_tensors.keys()):\n",
    "        Z = attention_tensors[hh]\n",
    "        ids = list(shopping_per_hh[hh][0].id.values())\n",
    "        E_update = model_fuse(ids, Z, f2)\n",
    "        E_updates[hh] = E_update\n",
    "    with open(path.join(\"..\\\\data\\\\pickles\", f\"E_updates_F1_{f1}_F2_{f2}.pkl.gz\"), \"wb\") as f:\n",
    "        pickle.dump(E_updates, f)\n",
    "else:\n",
    "    with open(path.join(\"..\\\\data\\\\pickles\", f\"E_updates_F1_{f1}_F2_{f2}.pkl.gz\"), \"rb\") as f:\n",
    "        E_updates = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class temporal_set_prediction(nn.Module):\n",
    "    def __init__(self, items_total, item_embedding_dim, reverse_uid):\n",
    "        \"\"\"\n",
    "        :param items_total: int\n",
    "        :param item_embedding_dim: int\n",
    "        :param n_heads: int\n",
    "        :param attention_aggregate: sre\n",
    "        \"\"\"\n",
    "        super(temporal_set_prediction, self).__init__()\n",
    "\n",
    "        ### To je njegov f0\n",
    "        self.item_embedding_dim = item_embedding_dim\n",
    "        \n",
    "        self.reverse_uid = reverse_uid\n",
    "        ## to je njegov num_all_unique_items\n",
    "        self.items_total = items_total\n",
    "        \n",
    "        \n",
    "        self.our_gcn = weighted_GCN(1, [256, 256], f1)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.stacked_gcn = stacked_weighted_GCN_blocks([weighted_GCN(item_embedding_dim,\n",
    "                                                                     [item_embedding_dim],\n",
    "                                                                     item_embedding_dim)])\n",
    "        \"\"\"\n",
    "\n",
    "        self.masked_self_attention = masked_self_attention_origi(input_dim=f1,\n",
    "                                                           output_dim=f2)\n",
    "\n",
    "        self.aggregate_nodes_temporal_feature = aggregate_nodes_temporal_feature_origi(f2)\n",
    "\n",
    "        \n",
    "        #\n",
    "        #(num_all_unique_items, f0, reverse_uid\n",
    "        #\n",
    "        self.global_gated_update = global_gated_update(items_total=self.items_total,\n",
    "                                                       f0=self.item_embedding_dim,\n",
    "                                                       item_dict=self.reverse_uid)\n",
    "\n",
    "        self.fc_output = nn.Sequential(nn.Linear(self.item_embedding_dim, 1, bias=True),\n",
    "                                       nn.Sigmoid())\n",
    "\n",
    "    \n",
    "    def forward(self, graph_list_for_hh):\n",
    "        embeddings_at_t = []\n",
    "        for graph in graph_list_for_hh:\n",
    "            o = model(graph.x,graph.edge_index)\n",
    "            embeddings_at_t.append(o)\n",
    "        item_embeddings = {j : [] for j in range(len(embeddings_at_t[0]))}\n",
    "        for t in range(len(embeddings_at_t)):\n",
    "            for j in range(len(embeddings_at_t[t])):\n",
    "                ## and add the embeddings for each item to its corresponding temporal index t in the newly created list\n",
    "                item_embeddings[j].append(embeddings_at_t[t][j].tolist())\n",
    "\n",
    "        ## convert the final 3D array to a tensor and save it to the dictionary for further use.\n",
    "        h = torch.tensor(list(item_embeddings.values()))\n",
    "\n",
    "        h = self.masked_self_attention(h)\n",
    "        h = self.aggregate_nodes_temporal_feature(h)\n",
    "        h = h[:,:,0]\n",
    "        ids = list(graph_list_for_hh[0].id.values())\n",
    "        h = self.global_gated_update(ids, h, self.item_embedding_dim)\n",
    "        out = self.fc_output(h).squeeze(dim=-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24375])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_model = temporal_set_prediction(num_all_unique_items, f0, reverse_uid)\n",
    "output = total_model(shopping_per_hh[\"1000\"])\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   1, 100,   3, 100,   5, 100, 100,   8,   9])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "a[[2, 4, 6,7]] = 100\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2240/731255353.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mitems_E\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_all_unique_items\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "E = torch.randn((num_all_unique_items, 32))\n",
    "E.size()\n",
    "\n",
    "items_E = E(torch.tensor([i for i in range(num_all_unique_items)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here is an attempt of me constructing a PyG dataset. It's not working as intended atm. \n",
    "## TODO or Deprecated methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Dataset\n",
    "class ShoppingDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return  os.listdir(\"../data/Test-Graphs/content/Graphs/\")\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'not_implemented.pt'\n",
    "\n",
    "    def download(self):\n",
    "        # Download to `self.raw_dir`.\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        idx = 0\n",
    "        for i, dat in enumerate(data_list):\n",
    "            torch.save(data, os.path.join(self.processed_dir, f'data_{i}.pt'))\n",
    "#         for raw_path in self.raw_paths:\n",
    "#             # Read data from `raw_path`.\n",
    "#             data = Data(...)\n",
    "\n",
    "#             if self.pre_filter is not None and not self.pre_filter(data):\n",
    "#                 continue\n",
    "\n",
    "#             if self.pre_transform is not None:\n",
    "#                 data = self.pre_transform(data)\n",
    "\n",
    "#             torch.save(data, osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "#             idx += 1\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        return data\n",
    "\n",
    "ds = ShoppingDataset(root='../data/ShoppingDataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: This still isn't used.\n",
    "## This class was used before for prepping our representations for the attention based temporal learning modulee. It's not used anywhere at the moment.\n",
    "class stacked_weighted_GCN_blocks(nn.ModuleList):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(stacked_weighted_GCN_blocks, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, *input):\n",
    "        nodes_feature, edge_weights = input\n",
    "        h = nodes_feature\n",
    "        for module in self:\n",
    "            h = module(h, edge_weights)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A PyG-attempt at masked_Self_attention\n",
    "class masked_self_attention(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, n_heads=4, concat = True):\n",
    "        super(masked_self_attention, self).__init__()\n",
    "        self.concat = concat\n",
    "\n",
    "        # the dimension of each head is dq // n_heads\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.W_x = nn.Linear(input_dim, output_dim)\n",
    "        self.transformer = torch_geometric.nn.TransformerConv(self.input_dim, self.output_dim, self.n_heads, concat = self.concat)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        output = self.transformer(x, edge_index)\n",
    "        output = output.matmul(self.W_x(x))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class aggregate_nodes_temporal_feature(nn.Module):\n",
    "    def __init__(self, item_embed_dim):\n",
    "        super(aggregate_nodes_temporal_feature, self).__init__()\n",
    "\n",
    "        self.Wq = nn.Linear(item_embed_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, Z):\n",
    "        output = nn.Linear(Z).t().matmul(Z).t()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLG-Project",
   "language": "python",
   "name": "mlg-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
